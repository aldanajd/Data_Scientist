{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claude\n",
    "\n",
    "in the python library statsmodels, there is a parameter called \"bse\" under OLS class, show me it's code\n",
    "\n",
    "in the python library statsmodels, there is a parameter called \"normalized_cov_params\" under OLS class, show me it's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pinv_extended(x, rcond=1e-15):\n",
    "    \"\"\"\n",
    "    Return the pinv of an array X as well as the singular values\n",
    "    used in computation.\n",
    "\n",
    "    Code adapted from numpy.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    x = x.conjugate()\n",
    "    u, s, vt = np.linalg.svd(x, False)\n",
    "    s_orig = np.copy(s)\n",
    "    m = u.shape[0]\n",
    "    n = vt.shape[1]\n",
    "    cutoff = rcond * np.maximum.reduce(s)\n",
    "    for i in range(min(n, m)):\n",
    "        if s[i] > cutoff:\n",
    "            s[i] = 1./s[i]\n",
    "        else:\n",
    "            s[i] = 0.\n",
    "    res = np.dot(np.transpose(vt), np.multiply(s[:, np.newaxis],\n",
    "                                               np.transpose(u)))\n",
    "    return res, s_orig\n",
    "\n",
    "pinv_wexog, singular_values = pinv_extended(self.wexog)\n",
    "\n",
    "# Calculate normalized_cov_params\n",
    "    self.pinv_wexog, singular_values = pinv_extended(self.wexog)\n",
    "    self.normalized_cov_params = np.dot(\n",
    "        self.pinv_wexog, np.transpose(self.pinv_wexog))\n",
    "\n",
    "    # Cache these singular values for use later.\n",
    "    self.wexog_singular_values = singular_values\n",
    "    self.rank = np.linalg.matrix_rank(np.diag(singular_values))\n",
    "\n",
    "def cov_params(self, r_matrix=None, column=None, scale=None, cov_p=None,\n",
    "                   other=None):\n",
    "        \"\"\"\n",
    "        Compute the variance/covariance matrix.\n",
    "\n",
    "        The variance/covariance matrix can be of a linear contrast of the\n",
    "        estimated parameters or all params multiplied by scale which will\n",
    "        usually be an estimate of sigma^2.  Scale is assumed to be a scalar.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        r_matrix : array_like\n",
    "            Can be 1d, or 2d.  Can be used alone or with other.\n",
    "        column : array_like, optional\n",
    "            Must be used on its own.  Can be 0d or 1d see below.\n",
    "        scale : float, optional\n",
    "            Can be specified or not.  Default is None, which means that\n",
    "            the scale argument is taken from the model.\n",
    "        cov_p : ndarray, optional\n",
    "            The covariance of the parameters. If not provided, this value is\n",
    "            read from `self.normalized_cov_params` or\n",
    "            `self.cov_params_default`.\n",
    "        other : array_like, optional\n",
    "            Can be used when r_matrix is specified.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The covariance matrix of the parameter estimates or of linear\n",
    "            combination of parameter estimates. See Notes.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        (The below are assumed to be in matrix notation.)\n",
    "\n",
    "        If no argument is specified returns the covariance matrix of a model\n",
    "        ``(scale)*(X.T X)^(-1)``\n",
    "\n",
    "        If contrast is specified it pre and post-multiplies as follows\n",
    "        ``(scale) * r_matrix (X.T X)^(-1) r_matrix.T``\n",
    "\n",
    "        If contrast and other are specified returns\n",
    "        ``(scale) * r_matrix (X.T X)^(-1) other.T``\n",
    "\n",
    "        If column is specified returns\n",
    "        ``(scale) * (X.T X)^(-1)[column,column]`` if column is 0d\n",
    "\n",
    "        OR\n",
    "\n",
    "        ``(scale) * (X.T X)^(-1)[column][:,column]`` if column is 1d\"\"\"\n",
    "\n",
    "        if cov_p is None:\n",
    "            if hasattr(self, 'cov_params_default'):\n",
    "                cov_p = self.cov_params_default\n",
    "            else:\n",
    "                if scale is None:\n",
    "                    scale = self.scale\n",
    "                cov_p = self.normalized_cov_params * scale\n",
    "\n",
    "            return cov_p\n",
    "        \n",
    "\n",
    "def bse(self):\n",
    "    \"\"\"\n",
    "    The standard errors of the parameter estimates.\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.diag(self.cov_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(base.LikelihoodModel):\n",
    "    \"\"\"\n",
    "    Base class for linear regression models. Should not be directly called.\n",
    "\n",
    "    Intended for subclassing.\n",
    "    \"\"\"\n",
    "    def __init__(self, endog, exog, **kwargs):\n",
    "        super().__init__(endog, exog, **kwargs)\n",
    "        self.pinv_wexog: Float64Array | None = None\n",
    "        self._data_attr.extend(['pinv_wexog', 'wendog', 'wexog', 'weights'])\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize model components.\"\"\"\n",
    "        self.wexog = self.whiten(self.exog)\n",
    "        self.wendog = self.whiten(self.endog)\n",
    "        # overwrite nobs from class Model:\n",
    "        self.nobs = float(self.wexog.shape[0])\n",
    "\n",
    "        self._df_model = None\n",
    "        self._df_resid = None\n",
    "        self.rank = None\n",
    "\n",
    "    @property\n",
    "    def df_model(self):\n",
    "        \"\"\"\n",
    "        The model degree of freedom.\n",
    "\n",
    "        The dof is defined as the rank of the regressor matrix minus 1 if a\n",
    "        constant is included.\n",
    "        \"\"\"\n",
    "        if self._df_model is None:\n",
    "            if self.rank is None:\n",
    "                self.rank = np.linalg.matrix_rank(self.exog)\n",
    "            self._df_model = float(self.rank - self.k_constant)\n",
    "        return self._df_model\n",
    "\n",
    "    @df_model.setter\n",
    "    def df_model(self, value):\n",
    "        self._df_model = value\n",
    "\n",
    "    @property\n",
    "    def df_resid(self):\n",
    "        \"\"\"\n",
    "        The residual degree of freedom.\n",
    "\n",
    "        The dof is defined as the number of observations minus the rank of\n",
    "        the regressor matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._df_resid is None:\n",
    "            if self.rank is None:\n",
    "                self.rank = np.linalg.matrix_rank(self.exog)\n",
    "            self._df_resid = self.nobs - self.rank\n",
    "        return self._df_resid\n",
    "\n",
    "    @df_resid.setter\n",
    "    def df_resid(self, value):\n",
    "        self._df_resid = value\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        Whiten method that must be overwritten by individual models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement.\")\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            method: Literal[\"pinv\", \"qr\"] = \"pinv\",\n",
    "            cov_type: Literal[\n",
    "                \"nonrobust\",\n",
    "                \"fixed scale\",\n",
    "                \"HC0\",\n",
    "                \"HC1\",\n",
    "                \"HC2\",\n",
    "                \"HC3\",\n",
    "                \"HAC\",\n",
    "                \"hac-panel\",\n",
    "                \"hac-groupsum\",\n",
    "                \"cluster\",\n",
    "            ] = \"nonrobust\",\n",
    "            cov_kwds=None,\n",
    "            use_t: bool | None = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Full fit of the model.\n",
    "\n",
    "        The results include an estimate of covariance matrix, (whitened)\n",
    "        residuals and an estimate of scale.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, optional\n",
    "            Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
    "            to solve the least squares problem. \"qr\" uses the QR\n",
    "            factorization.\n",
    "        cov_type : str, optional\n",
    "            See `regression.linear_model.RegressionResults` for a description\n",
    "            of the available covariance estimators.\n",
    "        cov_kwds : list or None, optional\n",
    "            See `linear_model.RegressionResults.get_robustcov_results` for a\n",
    "            description required keywords for alternative covariance\n",
    "            estimators.\n",
    "        use_t : bool, optional\n",
    "            Flag indicating to use the Student's t distribution when computing\n",
    "            p-values.  Default behavior depends on cov_type. See\n",
    "            `linear_model.RegressionResults.get_robustcov_results` for\n",
    "            implementation details.\n",
    "        **kwargs\n",
    "            Additional keyword arguments that contain information used when\n",
    "            constructing a model using the formula interface.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        RegressionResults\n",
    "            The model estimation results.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        RegressionResults\n",
    "            The results container.\n",
    "        RegressionResults.get_robustcov_results\n",
    "            A method to change the covariance estimator used when fitting the\n",
    "            model.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The fit method uses the pseudoinverse of the design/exogenous variables\n",
    "        to solve the least squares minimization.\n",
    "        \"\"\"\n",
    "        if method == \"pinv\":\n",
    "            if not (hasattr(self, 'pinv_wexog') and\n",
    "                    hasattr(self, 'normalized_cov_params') and\n",
    "                    hasattr(self, 'rank')):\n",
    "\n",
    "                self.pinv_wexog, singular_values = pinv_extended(self.wexog)\n",
    "                self.normalized_cov_params = np.dot(\n",
    "                    self.pinv_wexog, np.transpose(self.pinv_wexog))\n",
    "\n",
    "                # Cache these singular values for use later.\n",
    "                self.wexog_singular_values = singular_values\n",
    "                self.rank = np.linalg.matrix_rank(np.diag(singular_values))\n",
    "\n",
    "            beta = np.dot(self.pinv_wexog, self.wendog)\n",
    "\n",
    "        elif method == \"qr\":\n",
    "            if not (hasattr(self, 'exog_Q') and\n",
    "                    hasattr(self, 'exog_R') and\n",
    "                    hasattr(self, 'normalized_cov_params') and\n",
    "                    hasattr(self, 'rank')):\n",
    "                Q, R = np.linalg.qr(self.wexog)\n",
    "                self.exog_Q, self.exog_R = Q, R\n",
    "                self.normalized_cov_params = np.linalg.inv(np.dot(R.T, R))\n",
    "\n",
    "                # Cache singular values from R.\n",
    "                self.wexog_singular_values = np.linalg.svd(R, 0, 0)\n",
    "                self.rank = np.linalg.matrix_rank(R)\n",
    "            else:\n",
    "                Q, R = self.exog_Q, self.exog_R\n",
    "            # Needed for some covariance estimators, see GH #8157\n",
    "            self.pinv_wexog = np.linalg.pinv(self.wexog)\n",
    "            # used in ANOVA\n",
    "            self.effects = effects = np.dot(Q.T, self.wendog)\n",
    "            beta = np.linalg.solve(R, effects)\n",
    "        else:\n",
    "            raise ValueError('method has to be \"pinv\" or \"qr\"')\n",
    "\n",
    "        if self._df_model is None:\n",
    "            self._df_model = float(self.rank - self.k_constant)\n",
    "        if self._df_resid is None:\n",
    "            self.df_resid = self.nobs - self.rank\n",
    "\n",
    "        if isinstance(self, OLS):\n",
    "            lfit = OLSResults(\n",
    "                self, beta,\n",
    "                normalized_cov_params=self.normalized_cov_params,\n",
    "                cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n",
    "        else:\n",
    "            lfit = RegressionResults(\n",
    "                self, beta,\n",
    "                normalized_cov_params=self.normalized_cov_params,\n",
    "                cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t,\n",
    "                **kwargs)\n",
    "        return RegressionResultsWrapper(lfit)\n",
    "\n",
    "    def predict(self, params, exog=None):\n",
    "        \"\"\"\n",
    "        Return linear predicted values from a design matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            Parameters of a linear model.\n",
    "        exog : array_like, optional\n",
    "            Design / exogenous data. Model exog is used if None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            An array of fitted values.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        If the model has not yet been fit, params is not optional.\n",
    "        \"\"\"\n",
    "        # JP: this does not look correct for GLMAR\n",
    "        # SS: it needs its own predict method\n",
    "\n",
    "        if exog is None:\n",
    "            exog = self.exog\n",
    "\n",
    "        return np.dot(exog, params)\n",
    "\n",
    "    def get_distribution(self, params, scale, exog=None, dist_class=None):\n",
    "        \"\"\"\n",
    "        Construct a random number generator for the predictive distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The model parameters (regression coefficients).\n",
    "        scale : scalar\n",
    "            The variance parameter.\n",
    "        exog : array_like\n",
    "            The predictor variable matrix.\n",
    "        dist_class : class\n",
    "            A random number generator class.  Must take 'loc' and 'scale'\n",
    "            as arguments and return a random number generator implementing\n",
    "            an ``rvs`` method for simulating random values. Defaults to normal.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gen\n",
    "            Frozen random number generator object with mean and variance\n",
    "            determined by the fitted linear model.  Use the ``rvs`` method\n",
    "            to generate random values.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Due to the behavior of ``scipy.stats.distributions objects``,\n",
    "        the returned random number generator must be called with\n",
    "        ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
    "        the data set used to fit the model.  If any other value is\n",
    "        used for ``n``, misleading results will be produced.\n",
    "        \"\"\"\n",
    "        fit = self.predict(params, exog)\n",
    "        if dist_class is None:\n",
    "            from scipy.stats.distributions import norm\n",
    "            dist_class = norm\n",
    "        gen = dist_class(loc=fit, scale=np.sqrt(scale))\n",
    "        return gen\n",
    "\n",
    "class WLS(RegressionModel):\n",
    "    __doc__ = \"\"\"\n",
    "    Weighted Least Squares\n",
    "\n",
    "    The weights are presumed to be (proportional to) the inverse of\n",
    "    the variance of the observations.  That is, if the variables are\n",
    "    to be transformed by 1/sqrt(W) you must supply weights = 1/W.\n",
    "\n",
    "    {params}\n",
    "    weights : array_like, optional\n",
    "        A 1d array of weights.  If you supply 1/W then the variables are\n",
    "        pre- multiplied by 1/sqrt(W).  If no weights are supplied the\n",
    "        default value is 1 and WLS results are the same as OLS.\n",
    "    {extra_params}\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : ndarray\n",
    "        The stored weights supplied as an argument.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    GLS : Fit a linear model using Generalized Least Squares.\n",
    "    OLS : Fit a linear model using Ordinary Least Squares.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If the weights are a function of the data, then the post estimation\n",
    "    statistics such as fvalue and mse_model might not be correct, as the\n",
    "    package does not yet support no-constant regression.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import statsmodels.api as sm\n",
    "    >>> Y = [1,3,4,5,2,3,4]\n",
    "    >>> X = range(1,8)\n",
    "    >>> X = sm.add_constant(X)\n",
    "    >>> wls_model = sm.WLS(Y,X, weights=list(range(1,8)))\n",
    "    >>> results = wls_model.fit()\n",
    "    >>> results.params\n",
    "    array([ 2.91666667,  0.0952381 ])\n",
    "    >>> results.tvalues\n",
    "    array([ 2.0652652 ,  0.35684428])\n",
    "    >>> print(results.t_test([1, 0]))\n",
    "    <T test: effect=array([ 2.91666667]), sd=array([[ 1.41224801]]),\n",
    "     t=array([[ 2.0652652]]), p=array([[ 0.04690139]]), df_denom=5>\n",
    "    >>> print(results.f_test([0, 1]))\n",
    "    <F test: F=array([[ 0.12733784]]), p=[[ 0.73577409]], df_denom=5, df_num=1>\n",
    "    \"\"\".format(params=base._model_params_doc,\n",
    "           extra_params=base._missing_param_doc + base._extra_param_doc)\n",
    "\n",
    "    def __init__(self, endog, exog, weights=1., missing='none', hasconst=None,\n",
    "                 **kwargs):\n",
    "        if type(self) is WLS:\n",
    "            self._check_kwargs(kwargs)\n",
    "        weights = np.array(weights)\n",
    "        if weights.shape == ():\n",
    "            if (missing == 'drop' and 'missing_idx' in kwargs and\n",
    "                    kwargs['missing_idx'] is not None):\n",
    "                # patsy may have truncated endog\n",
    "                weights = np.repeat(weights, len(kwargs['missing_idx']))\n",
    "            else:\n",
    "                weights = np.repeat(weights, len(endog))\n",
    "        # handle case that endog might be of len == 1\n",
    "        if len(weights) == 1:\n",
    "            weights = np.array([weights.squeeze()])\n",
    "        else:\n",
    "            weights = weights.squeeze()\n",
    "        super().__init__(endog, exog, missing=missing,\n",
    "                                  weights=weights, hasconst=hasconst, **kwargs)\n",
    "        nobs = self.exog.shape[0]\n",
    "        weights = self.weights\n",
    "        if weights.size != nobs and weights.shape[0] != nobs:\n",
    "            raise ValueError('Weights must be scalar or same length as design')\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        Whitener for WLS model, multiplies each column by sqrt(self.weights).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The whitened values sqrt(weights)*X.\n",
    "        \"\"\"\n",
    "\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            return x * np.sqrt(self.weights)\n",
    "        elif x.ndim == 2:\n",
    "            return np.sqrt(self.weights)[:, None] * x\n",
    "\n",
    "    def loglike(self, params):\n",
    "        r\"\"\"\n",
    "        Compute the value of the gaussian log-likelihood function at params.\n",
    "\n",
    "        Given the whitened design matrix, the log-likelihood is evaluated\n",
    "        at the parameter vector `params` for the dependent variable `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The value of the log-likelihood function for a WLS Model.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        .. math:: -\\frac{n}{2}\\log SSR\n",
    "                  -\\frac{n}{2}\\left(1+\\log\\left(\\frac{2\\pi}{n}\\right)\\right)\n",
    "                  +\\frac{1}{2}\\log\\left(\\left|W\\right|\\right)\n",
    "\n",
    "        where :math:`W` is a diagonal weight matrix,\n",
    "        :math:`\\left|W\\right|` is its determinant, and\n",
    "        :math:`SSR=\\left(Y-\\hat{Y}\\right)^\\prime W \\left(Y-\\hat{Y}\\right)` is\n",
    "        the sum of the squared weighted residuals.\n",
    "        \"\"\"\n",
    "        nobs2 = self.nobs / 2.0\n",
    "        SSR = np.sum((self.wendog - np.dot(self.wexog, params))**2, axis=0)\n",
    "        llf = -np.log(SSR) * nobs2      # concentrated likelihood\n",
    "        llf -= (1+np.log(np.pi/nobs2))*nobs2  # with constant\n",
    "        llf += 0.5 * np.sum(np.log(self.weights))\n",
    "        return llf\n",
    "\n",
    "    def hessian_factor(self, params, scale=None, observed=True):\n",
    "        \"\"\"\n",
    "        Compute the weights for calculating the Hessian.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            The parameter at which Hessian is evaluated.\n",
    "        scale : None or float\n",
    "            If scale is None, then the default scale will be calculated.\n",
    "            Default scale is defined by `self.scaletype` and set in fit.\n",
    "            If scale is not None, then it is used as a fixed scale.\n",
    "        observed : bool\n",
    "            If True, then the observed Hessian is returned. If false then the\n",
    "            expected information matrix is returned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            A 1d weight vector used in the calculation of the Hessian.\n",
    "            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.weights\n",
    "\n",
    "    @Appender(_fit_regularized_doc)\n",
    "    def fit_regularized(self, method=\"elastic_net\", alpha=0.,\n",
    "                        L1_wt=1., start_params=None, profile_scale=False,\n",
    "                        refit=False, **kwargs):\n",
    "        # Docstring attached below\n",
    "        if not np.isscalar(alpha):\n",
    "            alpha = np.asarray(alpha)\n",
    "        # Need to adjust since RSS/n in elastic net uses nominal n in\n",
    "        # denominator\n",
    "        alpha = alpha * np.sum(self.weights) / len(self.weights)\n",
    "\n",
    "        rslt = OLS(self.wendog, self.wexog).fit_regularized(\n",
    "            method=method, alpha=alpha,\n",
    "            L1_wt=L1_wt,\n",
    "            start_params=start_params,\n",
    "            profile_scale=profile_scale,\n",
    "            refit=refit, **kwargs)\n",
    "\n",
    "        from statsmodels.base.elastic_net import (\n",
    "            RegularizedResults,\n",
    "            RegularizedResultsWrapper,\n",
    "        )\n",
    "        rrslt = RegularizedResults(self, rslt.params)\n",
    "        return RegularizedResultsWrapper(rrslt)\n",
    "\n",
    "\n",
    "class OLS(WLS):\n",
    "    __doc__ = \"\"\"\n",
    "    Ordinary Least Squares\n",
    "\n",
    "    {params}\n",
    "    {extra_params}\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : scalar\n",
    "        Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    WLS : Fit a linear model using Weighted Least Squares.\n",
    "    GLS : Fit a linear model using Generalized Least Squares.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    No constant is added by the model unless you are using formulas.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import statsmodels.api as sm\n",
    "    >>> import numpy as np\n",
    "    >>> duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
    "    >>> Y = duncan_prestige.data['income']\n",
    "    >>> X = duncan_prestige.data['education']\n",
    "    >>> X = sm.add_constant(X)\n",
    "    >>> model = sm.OLS(Y,X)\n",
    "    >>> results = model.fit()\n",
    "    >>> results.params\n",
    "    const        10.603498\n",
    "    education     0.594859\n",
    "    dtype: float64\n",
    "\n",
    "    >>> results.tvalues\n",
    "    const        2.039813\n",
    "    education    6.892802\n",
    "    dtype: float64\n",
    "\n",
    "    >>> print(results.t_test([1, 0]))\n",
    "                                 Test for Constraints\n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    c0            10.6035      5.198      2.040      0.048       0.120      21.087\n",
    "    ==============================================================================\n",
    "\n",
    "    >>> print(results.f_test(np.identity(2)))\n",
    "    <F test: F=array([[159.63031026]]), p=1.2607168903696672e-20,\n",
    "     df_denom=43, df_num=2>\n",
    "    \"\"\".format(params=base._model_params_doc,\n",
    "           extra_params=base._missing_param_doc + base._extra_param_doc)\n",
    "\n",
    "    def __init__(self, endog, exog=None, missing='none', hasconst=None,\n",
    "                 **kwargs):\n",
    "        if \"weights\" in kwargs:\n",
    "            msg = (\"Weights are not supported in OLS and will be ignored\"\n",
    "                   \"An exception will be raised in the next version.\")\n",
    "            warnings.warn(msg, ValueWarning)\n",
    "        super().__init__(endog, exog, missing=missing,\n",
    "                                  hasconst=hasconst, **kwargs)\n",
    "        if \"weights\" in self._init_keys:\n",
    "            self._init_keys.remove(\"weights\")\n",
    "\n",
    "        if type(self) is OLS:\n",
    "            self._check_kwargs(kwargs, [\"offset\"])\n",
    "\n",
    "    def loglike(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        The likelihood function for the OLS model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The coefficients with which to estimate the log-likelihood.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The likelihood function evaluated at params.\n",
    "        \"\"\"\n",
    "        nobs2 = self.nobs / 2.0\n",
    "        nobs = float(self.nobs)\n",
    "        resid = self.endog - np.dot(self.exog, params)\n",
    "        if hasattr(self, 'offset'):\n",
    "            resid -= self.offset\n",
    "        ssr = np.sum(resid**2)\n",
    "        if scale is None:\n",
    "            # profile log likelihood\n",
    "            llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr / nobs) - nobs2\n",
    "        else:\n",
    "            # log-likelihood\n",
    "            llf = -nobs2 * np.log(2 * np.pi * scale) - ssr / (2*scale)\n",
    "        return llf\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        OLS model whitener does nothing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The input array unmodified.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        OLS : Fit a linear model using Ordinary Least Squares.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    def score(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        Evaluate the score function at a given point.\n",
    "\n",
    "        The score corresponds to the profile (concentrated)\n",
    "        log-likelihood in which the scale parameter has been profiled\n",
    "        out.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter vector at which the score function is\n",
    "            computed.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The score vector.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_wexog_xprod\"):\n",
    "            self._setup_score_hess()\n",
    "\n",
    "        xtxb = np.dot(self._wexog_xprod, params)\n",
    "        sdr = -self._wexog_x_wendog + xtxb\n",
    "\n",
    "        if scale is None:\n",
    "            ssr = self._wendog_xprod - 2 * np.dot(self._wexog_x_wendog.T,\n",
    "                                                  params)\n",
    "            ssr += np.dot(params, xtxb)\n",
    "            return -self.nobs * sdr / ssr\n",
    "        else:\n",
    "            return -sdr / scale\n",
    "\n",
    "    def _setup_score_hess(self):\n",
    "        y = self.wendog\n",
    "        if hasattr(self, 'offset'):\n",
    "            y = y - self.offset\n",
    "        self._wendog_xprod = np.sum(y * y)\n",
    "        self._wexog_xprod = np.dot(self.wexog.T, self.wexog)\n",
    "        self._wexog_x_wendog = np.dot(self.wexog.T, y)\n",
    "\n",
    "    def hessian(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        Evaluate the Hessian function at a given point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter vector at which the Hessian is computed.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The Hessian matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_wexog_xprod\"):\n",
    "            self._setup_score_hess()\n",
    "\n",
    "        xtxb = np.dot(self._wexog_xprod, params)\n",
    "\n",
    "        if scale is None:\n",
    "            ssr = self._wendog_xprod - 2 * np.dot(self._wexog_x_wendog.T,\n",
    "                                                  params)\n",
    "            ssr += np.dot(params, xtxb)\n",
    "            ssrp = -2*self._wexog_x_wendog + 2*xtxb\n",
    "            hm = self._wexog_xprod / ssr - np.outer(ssrp, ssrp) / ssr**2\n",
    "            return -self.nobs * hm / 2\n",
    "        else:\n",
    "            return -self._wexog_xprod / scale\n",
    "\n",
    "    def hessian_factor(self, params, scale=None, observed=True):\n",
    "        \"\"\"\n",
    "        Calculate the weights for the Hessian.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            The parameter at which Hessian is evaluated.\n",
    "        scale : None or float\n",
    "            If scale is None, then the default scale will be calculated.\n",
    "            Default scale is defined by `self.scaletype` and set in fit.\n",
    "            If scale is not None, then it is used as a fixed scale.\n",
    "        observed : bool\n",
    "            If True, then the observed Hessian is returned. If false then the\n",
    "            expected information matrix is returned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            A 1d weight vector used in the calculation of the Hessian.\n",
    "            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.ones(self.exog.shape[0])\n",
    "\n",
    "    @Appender(_fit_regularized_doc)\n",
    "    def fit_regularized(self, method=\"elastic_net\", alpha=0.,\n",
    "                        L1_wt=1., start_params=None, profile_scale=False,\n",
    "                        refit=False, **kwargs):\n",
    "\n",
    "        # In the future we could add support for other penalties, e.g. SCAD.\n",
    "        if method not in (\"elastic_net\", \"sqrt_lasso\"):\n",
    "            msg = \"Unknown method '%s' for fit_regularized\" % method\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # Set default parameters.\n",
    "        defaults = {\"maxiter\":  50, \"cnvrg_tol\": 1e-10,\n",
    "                    \"zero_tol\": 1e-8}\n",
    "        defaults.update(kwargs)\n",
    "\n",
    "        if method == \"sqrt_lasso\":\n",
    "            from statsmodels.base.elastic_net import (\n",
    "                RegularizedResults,\n",
    "                RegularizedResultsWrapper,\n",
    "            )\n",
    "            params = self._sqrt_lasso(alpha, refit, defaults[\"zero_tol\"])\n",
    "            results = RegularizedResults(self, params)\n",
    "            return RegularizedResultsWrapper(results)\n",
    "\n",
    "        from statsmodels.base.elastic_net import fit_elasticnet\n",
    "\n",
    "        if L1_wt == 0:\n",
    "            return self._fit_ridge(alpha)\n",
    "\n",
    "        # If a scale parameter is passed in, the non-profile\n",
    "        # likelihood (residual sum of squares divided by -2) is used,\n",
    "        # otherwise the profile likelihood is used.\n",
    "        if profile_scale:\n",
    "            loglike_kwds = {}\n",
    "            score_kwds = {}\n",
    "            hess_kwds = {}\n",
    "        else:\n",
    "            loglike_kwds = {\"scale\": 1}\n",
    "            score_kwds = {\"scale\": 1}\n",
    "            hess_kwds = {\"scale\": 1}\n",
    "\n",
    "        return fit_elasticnet(self, method=method,\n",
    "                              alpha=alpha,\n",
    "                              L1_wt=L1_wt,\n",
    "                              start_params=start_params,\n",
    "                              loglike_kwds=loglike_kwds,\n",
    "                              score_kwds=score_kwds,\n",
    "                              hess_kwds=hess_kwds,\n",
    "                              refit=refit,\n",
    "                              check_step=False,\n",
    "                              **defaults)\n",
    "\n",
    "    def _sqrt_lasso(self, alpha, refit, zero_tol):\n",
    "\n",
    "        try:\n",
    "            import cvxopt\n",
    "        except ImportError:\n",
    "            msg = 'sqrt_lasso fitting requires the cvxopt module'\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        n = len(self.endog)\n",
    "        p = self.exog.shape[1]\n",
    "\n",
    "        h0 = cvxopt.matrix(0., (2*p+1, 1))\n",
    "        h1 = cvxopt.matrix(0., (n+1, 1))\n",
    "        h1[1:, 0] = cvxopt.matrix(self.endog, (n, 1))\n",
    "\n",
    "        G0 = cvxopt.spmatrix([], [], [], (2*p+1, 2*p+1))\n",
    "        for i in range(1, 2*p+1):\n",
    "            G0[i, i] = -1\n",
    "        G1 = cvxopt.matrix(0., (n+1, 2*p+1))\n",
    "        G1[0, 0] = -1\n",
    "        G1[1:, 1:p+1] = self.exog\n",
    "        G1[1:, p+1:] = -self.exog\n",
    "\n",
    "        c = cvxopt.matrix(alpha / n, (2*p + 1, 1))\n",
    "        c[0] = 1 / np.sqrt(n)\n",
    "\n",
    "        from cvxopt import solvers\n",
    "        solvers.options[\"show_progress\"] = False\n",
    "\n",
    "        rslt = solvers.socp(c, Gl=G0, hl=h0, Gq=[G1], hq=[h1])\n",
    "        x = np.asarray(rslt['x']).flat\n",
    "        bp = x[1:p+1]\n",
    "        bn = x[p+1:]\n",
    "        params = bp - bn\n",
    "\n",
    "        if not refit:\n",
    "            return params\n",
    "\n",
    "        ii = np.flatnonzero(np.abs(params) > zero_tol)\n",
    "        rfr = OLS(self.endog, self.exog[:, ii]).fit()\n",
    "        params *= 0\n",
    "        params[ii] = rfr.params\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _fit_ridge(self, alpha):\n",
    "        \"\"\"\n",
    "        Fit a linear model using ridge regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : scalar or array_like\n",
    "            The penalty weight.  If a scalar, the same penalty weight\n",
    "            applies to all variables in the model.  If a vector, it\n",
    "            must have the same length as `params`, and contains a\n",
    "            penalty weight for each coefficient.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Equivalent to fit_regularized with L1_wt = 0 (but implemented\n",
    "        more efficiently).\n",
    "        \"\"\"\n",
    "\n",
    "        u, s, vt = np.linalg.svd(self.exog, 0)\n",
    "        v = vt.T\n",
    "        q = np.dot(u.T, self.endog) * s\n",
    "        s2 = s * s\n",
    "        if np.isscalar(alpha):\n",
    "            sd = s2 + alpha * self.nobs\n",
    "            params = q / sd\n",
    "            params = np.dot(v, params)\n",
    "        else:\n",
    "            alpha = np.asarray(alpha)\n",
    "            vtav = self.nobs * np.dot(vt, alpha[:, None] * v)\n",
    "            d = np.diag(vtav) + s2\n",
    "            np.fill_diagonal(vtav, d)\n",
    "            r = np.linalg.solve(vtav, q)\n",
    "            params = np.dot(v, r)\n",
    "\n",
    "        from statsmodels.base.elastic_net import RegularizedResults\n",
    "        return RegularizedResults(self, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionResults(base.LikelihoodModelResults):\n",
    "    r\"\"\"\n",
    "    This class summarizes the fit of a linear regression model.\n",
    "\n",
    "    It handles the output of contrasts, estimates of covariance, etc.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RegressionModel\n",
    "        The regression model instance.\n",
    "    params : ndarray\n",
    "        The estimated parameters.\n",
    "    normalized_cov_params : ndarray\n",
    "        The normalized covariance parameters.\n",
    "    scale : float\n",
    "        The estimated scale of the residuals.\n",
    "    cov_type : str\n",
    "        The covariance estimator used in the results.\n",
    "    cov_kwds : dict\n",
    "        Additional keywords used in the covariance specification.\n",
    "    use_t : bool\n",
    "        Flag indicating to use the Student's t in inference.\n",
    "    **kwargs\n",
    "        Additional keyword arguments used to initialize the results.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    pinv_wexog\n",
    "        See model class docstring for implementation details.\n",
    "    cov_type\n",
    "        Parameter covariance estimator used for standard errors and t-stats.\n",
    "    df_model\n",
    "        Model degrees of freedom. The number of regressors `p`. Does not\n",
    "        include the constant if one is present.\n",
    "    df_resid\n",
    "        Residual degrees of freedom. `n - p - 1`, if a constant is present.\n",
    "        `n - p` if a constant is not included.\n",
    "    het_scale\n",
    "        adjusted squared residuals for heteroscedasticity robust standard\n",
    "        errors. Is only available after `HC#_se` or `cov_HC#` is called.\n",
    "        See HC#_se for more information.\n",
    "    history\n",
    "        Estimation history for iterative estimators.\n",
    "    model\n",
    "        A pointer to the model instance that called fit() or results.\n",
    "    params\n",
    "        The linear coefficients that minimize the least squares\n",
    "        criterion.  This is usually called Beta for the classical\n",
    "        linear model.\n",
    "    \"\"\"\n",
    "\n",
    "    _cache = {}  # needs to be a class attribute for scale setter?\n",
    "\n",
    "    def __init__(self, model, params, normalized_cov_params=None, scale=1.,\n",
    "                 cov_type='nonrobust', cov_kwds=None, use_t=None, **kwargs):\n",
    "        super().__init__(\n",
    "            model, params, normalized_cov_params, scale)\n",
    "\n",
    "        self._cache = {}\n",
    "        if hasattr(model, 'wexog_singular_values'):\n",
    "            self._wexog_singular_values = model.wexog_singular_values\n",
    "        else:\n",
    "            self._wexog_singular_values = None\n",
    "\n",
    "        self.df_model = model.df_model\n",
    "        self.df_resid = model.df_resid\n",
    "\n",
    "        if cov_type == 'nonrobust':\n",
    "            self.cov_type = 'nonrobust'\n",
    "            self.cov_kwds = {\n",
    "                'description': 'Standard Errors assume that the ' +\n",
    "                'covariance matrix of the errors is correctly ' +\n",
    "                'specified.'}\n",
    "            if use_t is None:\n",
    "                use_t = True  # TODO: class default\n",
    "            self.use_t = use_t\n",
    "        else:\n",
    "            if cov_kwds is None:\n",
    "                cov_kwds = {}\n",
    "            if 'use_t' in cov_kwds:\n",
    "                # TODO: we want to get rid of 'use_t' in cov_kwds\n",
    "                use_t_2 = cov_kwds.pop('use_t')\n",
    "                if use_t is None:\n",
    "                    use_t = use_t_2\n",
    "                # TODO: warn or not?\n",
    "            self.get_robustcov_results(cov_type=cov_type, use_self=True,\n",
    "                                       use_t=use_t, **cov_kwds)\n",
    "        for key in kwargs:\n",
    "            setattr(self, key, kwargs[key])\n",
    "\n",
    "    def conf_int(self, alpha=.05, cols=None):\n",
    "        \"\"\"\n",
    "        Compute the confidence interval of the fitted parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float, optional\n",
    "            The `alpha` level for the confidence interval. The default\n",
    "            `alpha` = .05 returns a 95% confidence interval.\n",
    "        cols : array_like, optional\n",
    "            Columns to include in returned confidence intervals.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The confidence intervals.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The confidence interval is based on Student's t-distribution.\n",
    "        \"\"\"\n",
    "        # keep method for docstring for now\n",
    "        ci = super().conf_int(alpha=alpha, cols=cols)\n",
    "        return ci\n",
    "\n",
    "    @cache_readonly\n",
    "    def nobs(self):\n",
    "        \"\"\"Number of observations n.\"\"\"\n",
    "        return float(self.model.wexog.shape[0])\n",
    "\n",
    "    @cache_readonly\n",
    "    def fittedvalues(self):\n",
    "        \"\"\"The predicted values for the original (unwhitened) design.\"\"\"\n",
    "        return self.model.predict(self.params, self.model.exog)\n",
    "\n",
    "    @cache_readonly\n",
    "    def wresid(self):\n",
    "        \"\"\"\n",
    "        The residuals of the transformed/whitened regressand and regressor(s).\n",
    "        \"\"\"\n",
    "        return self.model.wendog - self.model.predict(\n",
    "            self.params, self.model.wexog)\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid(self):\n",
    "        \"\"\"The residuals of the model.\"\"\"\n",
    "        return self.model.endog - self.model.predict(\n",
    "            self.params, self.model.exog)\n",
    "\n",
    "    # TODO: fix writable example\n",
    "    @cache_writable()\n",
    "    def scale(self):\n",
    "        \"\"\"\n",
    "        A scale factor for the covariance matrix.\n",
    "\n",
    "        The Default value is ssr/(n-p).  Note that the square root of `scale`\n",
    "        is often called the standard error of the regression.\n",
    "        \"\"\"\n",
    "        wresid = self.wresid\n",
    "        return np.dot(wresid, wresid) / self.df_resid\n",
    "\n",
    "    @cache_readonly\n",
    "    def ssr(self):\n",
    "        \"\"\"Sum of squared (whitened) residuals.\"\"\"\n",
    "        wresid = self.wresid\n",
    "        return np.dot(wresid, wresid)\n",
    "\n",
    "    @cache_readonly\n",
    "    def centered_tss(self):\n",
    "        \"\"\"The total (weighted) sum of squares centered about the mean.\"\"\"\n",
    "        model = self.model\n",
    "        weights = getattr(model, 'weights', None)\n",
    "        sigma = getattr(model, 'sigma', None)\n",
    "        if weights is not None:\n",
    "            mean = np.average(model.endog, weights=weights)\n",
    "            return np.sum(weights * (model.endog - mean)**2)\n",
    "        elif sigma is not None:\n",
    "            # Exactly matches WLS when sigma is diagonal\n",
    "            iota = np.ones_like(model.endog)\n",
    "            iota = model.whiten(iota)\n",
    "            mean = model.wendog.dot(iota) / iota.dot(iota)\n",
    "            err = model.endog - mean\n",
    "            err = model.whiten(err)\n",
    "            return np.sum(err**2)\n",
    "        else:\n",
    "            centered_endog = model.wendog - model.wendog.mean()\n",
    "            return np.dot(centered_endog, centered_endog)\n",
    "\n",
    "    @cache_readonly\n",
    "    def uncentered_tss(self):\n",
    "        \"\"\"\n",
    "        Uncentered sum of squares.\n",
    "\n",
    "        The sum of the squared values of the (whitened) endogenous response\n",
    "        variable.\n",
    "        \"\"\"\n",
    "        wendog = self.model.wendog\n",
    "        return np.dot(wendog, wendog)\n",
    "\n",
    "    @cache_readonly\n",
    "    def ess(self):\n",
    "        \"\"\"\n",
    "        The explained sum of squares.\n",
    "\n",
    "        If a constant is present, the centered total sum of squares minus the\n",
    "        sum of squared residuals. If there is no constant, the uncentered total\n",
    "        sum of squares is used.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.k_constant:\n",
    "            return self.centered_tss - self.ssr\n",
    "        else:\n",
    "            return self.uncentered_tss - self.ssr\n",
    "\n",
    "    @cache_readonly\n",
    "    def rsquared(self):\n",
    "        \"\"\"\n",
    "        R-squared of the model.\n",
    "\n",
    "        This is defined here as 1 - `ssr`/`centered_tss` if the constant is\n",
    "        included in the model and 1 - `ssr`/`uncentered_tss` if the constant is\n",
    "        omitted.\n",
    "        \"\"\"\n",
    "        if self.k_constant:\n",
    "            return 1 - self.ssr/self.centered_tss\n",
    "        else:\n",
    "            return 1 - self.ssr/self.uncentered_tss\n",
    "\n",
    "    @cache_readonly\n",
    "    def rsquared_adj(self):\n",
    "        \"\"\"\n",
    "        Adjusted R-squared.\n",
    "\n",
    "        This is defined here as 1 - (`nobs`-1)/`df_resid` * (1-`rsquared`)\n",
    "        if a constant is included and 1 - `nobs`/`df_resid` * (1-`rsquared`) if\n",
    "        no constant is included.\n",
    "        \"\"\"\n",
    "        return 1 - (np.divide(self.nobs - self.k_constant, self.df_resid)\n",
    "                    * (1 - self.rsquared))\n",
    "\n",
    "    @cache_readonly\n",
    "    def mse_model(self):\n",
    "        \"\"\"\n",
    "        Mean squared error the model.\n",
    "\n",
    "        The explained sum of squares divided by the model degrees of freedom.\n",
    "        \"\"\"\n",
    "        if np.all(self.df_model == 0.0):\n",
    "            return np.full_like(self.ess, np.nan)\n",
    "        return self.ess/self.df_model\n",
    "\n",
    "    @cache_readonly\n",
    "    def mse_resid(self):\n",
    "        \"\"\"\n",
    "        Mean squared error of the residuals.\n",
    "\n",
    "        The sum of squared residuals divided by the residual degrees of\n",
    "        freedom.\n",
    "        \"\"\"\n",
    "        if np.all(self.df_resid == 0.0):\n",
    "            return np.full_like(self.ssr, np.nan)\n",
    "        return self.ssr/self.df_resid\n",
    "\n",
    "    @cache_readonly\n",
    "    def mse_total(self):\n",
    "        \"\"\"\n",
    "        Total mean squared error.\n",
    "\n",
    "        The uncentered total sum of squares divided by the number of\n",
    "        observations.\n",
    "        \"\"\"\n",
    "        if np.all(self.df_resid + self.df_model == 0.0):\n",
    "            return np.full_like(self.centered_tss, np.nan)\n",
    "        if self.k_constant:\n",
    "            return self.centered_tss / (self.df_resid + self.df_model)\n",
    "        else:\n",
    "            return self.uncentered_tss / (self.df_resid + self.df_model)\n",
    "\n",
    "    @cache_readonly\n",
    "    def fvalue(self):\n",
    "        \"\"\"\n",
    "        F-statistic of the fully specified model.\n",
    "\n",
    "        Calculated as the mean squared error of the model divided by the mean\n",
    "        squared error of the residuals if the nonrobust covariance is used.\n",
    "        Otherwise computed using a Wald-like quadratic form that tests whether\n",
    "        all coefficients (excluding the constant) are zero.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'cov_type') and self.cov_type != 'nonrobust':\n",
    "            # with heteroscedasticity or correlation robustness\n",
    "            k_params = self.normalized_cov_params.shape[0]\n",
    "            mat = np.eye(k_params)\n",
    "            const_idx = self.model.data.const_idx\n",
    "            # TODO: What if model includes implicit constant, e.g. all\n",
    "            #       dummies but no constant regressor?\n",
    "            # TODO: Restats as LM test by projecting orthogonalizing\n",
    "            #       to constant?\n",
    "            if self.model.data.k_constant == 1:\n",
    "                # if constant is implicit, return nan see #2444\n",
    "                if const_idx is None:\n",
    "                    return np.nan\n",
    "\n",
    "                idx = lrange(k_params)\n",
    "                idx.pop(const_idx)\n",
    "                mat = mat[idx]  # remove constant\n",
    "                if mat.size == 0:  # see  #3642\n",
    "                    return np.nan\n",
    "            ft = self.f_test(mat)\n",
    "            # using backdoor to set another attribute that we already have\n",
    "            self._cache['f_pvalue'] = float(ft.pvalue)\n",
    "            return float(ft.fvalue)\n",
    "        else:\n",
    "            # for standard homoscedastic case\n",
    "            return self.mse_model/self.mse_resid\n",
    "\n",
    "    @cache_readonly\n",
    "    def f_pvalue(self):\n",
    "        \"\"\"The p-value of the F-statistic.\"\"\"\n",
    "        # Special case for df_model 0\n",
    "        if self.df_model == 0:\n",
    "            return np.full_like(self.fvalue, np.nan)\n",
    "        return stats.f.sf(self.fvalue, self.df_model, self.df_resid)\n",
    "\n",
    "    @cache_readonly\n",
    "    def bse(self):\n",
    "        \"\"\"The standard errors of the parameter estimates.\"\"\"\n",
    "        return np.sqrt(np.diag(self.cov_params()))\n",
    "\n",
    "    @cache_readonly\n",
    "    def aic(self):\n",
    "        r\"\"\"\n",
    "        Akaike's information criteria.\n",
    "\n",
    "        For a model with a constant :math:`-2llf + 2(df\\_model + 1)`. For a\n",
    "        model without a constant :math:`-2llf + 2(df\\_model)`.\n",
    "        \"\"\"\n",
    "        return self.info_criteria(\"aic\")\n",
    "\n",
    "    @cache_readonly\n",
    "    def bic(self):\n",
    "        r\"\"\"\n",
    "        Bayes' information criteria.\n",
    "\n",
    "        For a model with a constant :math:`-2llf + \\log(n)(df\\_model+1)`.\n",
    "        For a model without a constant :math:`-2llf + \\log(n)(df\\_model)`.\n",
    "        \"\"\"\n",
    "        return self.info_criteria(\"bic\")\n",
    "\n",
    "    def info_criteria(self, crit, dk_params=0):\n",
    "        \"\"\"Return an information criterion for the model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        crit : string\n",
    "            One of 'aic', 'bic', 'aicc' or 'hqic'.\n",
    "        dk_params : int or float\n",
    "            Correction to the number of parameters used in the information\n",
    "            criterion. By default, only mean parameters are included, the\n",
    "            scale parameter is not included in the parameter count.\n",
    "            Use ``dk_params=1`` to include scale in the parameter count.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Value of information criterion.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        Burnham KP, Anderson KR (2002). Model Selection and Multimodel\n",
    "        Inference; Springer New York.\n",
    "        \"\"\"\n",
    "        crit = crit.lower()\n",
    "        k_params = self.df_model + self.k_constant + dk_params\n",
    "\n",
    "        if crit == \"aic\":\n",
    "            return -2 * self.llf + 2 * k_params\n",
    "        elif crit == \"bic\":\n",
    "            bic = -2*self.llf + np.log(self.nobs) * k_params\n",
    "            return bic\n",
    "        elif crit == \"aicc\":\n",
    "            from statsmodels.tools.eval_measures import aicc\n",
    "            return aicc(self.llf, self.nobs, k_params)\n",
    "        elif crit == \"hqic\":\n",
    "            from statsmodels.tools.eval_measures import hqic\n",
    "            return hqic(self.llf, self.nobs, k_params)\n",
    "\n",
    "    @cache_readonly\n",
    "    def eigenvals(self):\n",
    "        \"\"\"\n",
    "        Return eigenvalues sorted in decreasing order.\n",
    "        \"\"\"\n",
    "        if self._wexog_singular_values is not None:\n",
    "            eigvals = self._wexog_singular_values ** 2\n",
    "        else:\n",
    "            wx = self.model.wexog\n",
    "            eigvals = np.linalg.eigvalsh(wx.T @ wx)\n",
    "        return np.sort(eigvals)[::-1]\n",
    "\n",
    "    @cache_readonly\n",
    "    def condition_number(self):\n",
    "        \"\"\"\n",
    "        Return condition number of exogenous matrix.\n",
    "\n",
    "        Calculated as ratio of largest to smallest singular value of the\n",
    "        exogenous variables. This value is the same as the square root of\n",
    "        the ratio of the largest to smallest eigenvalue of the inner-product\n",
    "        of the exogenous variables.\n",
    "        \"\"\"\n",
    "        eigvals = self.eigenvals\n",
    "        return np.sqrt(eigvals[0]/eigvals[-1])\n",
    "\n",
    "    # TODO: make these properties reset bse\n",
    "    def _HCCM(self, scale):\n",
    "        H = np.dot(self.model.pinv_wexog,\n",
    "                   scale[:, None] * self.model.pinv_wexog.T)\n",
    "        return H\n",
    "\n",
    "    def _abat_diagonal(self, a, b):\n",
    "        # equivalent to np.diag(a @ b @ a.T)\n",
    "        return np.einsum('ij,ik,kj->i', a, a, b)\n",
    "\n",
    "    @cache_readonly\n",
    "    def cov_HC0(self):\n",
    "        \"\"\"\n",
    "        Heteroscedasticity robust covariance matrix. See HC0_se.\n",
    "        \"\"\"\n",
    "        self.het_scale = self.wresid**2\n",
    "        cov_HC0 = self._HCCM(self.het_scale)\n",
    "        return cov_HC0\n",
    "\n",
    "    @cache_readonly\n",
    "    def cov_HC1(self):\n",
    "        \"\"\"\n",
    "        Heteroscedasticity robust covariance matrix. See HC1_se.\n",
    "        \"\"\"\n",
    "        self.het_scale = self.nobs/(self.df_resid)*(self.wresid**2)\n",
    "        cov_HC1 = self._HCCM(self.het_scale)\n",
    "        return cov_HC1\n",
    "\n",
    "    @cache_readonly\n",
    "    def cov_HC2(self):\n",
    "        \"\"\"\n",
    "        Heteroscedasticity robust covariance matrix. See HC2_se.\n",
    "        \"\"\"\n",
    "        wexog = self.model.wexog\n",
    "        h = self._abat_diagonal(wexog, self.normalized_cov_params)\n",
    "        self.het_scale = self.wresid**2/(1-h)\n",
    "        cov_HC2 = self._HCCM(self.het_scale)\n",
    "        return cov_HC2\n",
    "\n",
    "    @cache_readonly\n",
    "    def cov_HC3(self):\n",
    "        \"\"\"\n",
    "        Heteroscedasticity robust covariance matrix. See HC3_se.\n",
    "        \"\"\"\n",
    "        wexog = self.model.wexog\n",
    "        h = self._abat_diagonal(wexog, self.normalized_cov_params)\n",
    "        self.het_scale = (self.wresid / (1 - h))**2\n",
    "        cov_HC3 = self._HCCM(self.het_scale)\n",
    "        return cov_HC3\n",
    "\n",
    "    @cache_readonly\n",
    "    def HC0_se(self):\n",
    "        \"\"\"\n",
    "        White's (1980) heteroskedasticity robust standard errors.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Defined as sqrt(diag(X.T X)^(-1)X.T diag(e_i^(2)) X(X.T X)^(-1)\n",
    "        where e_i = resid[i].\n",
    "\n",
    "        When HC0_se or cov_HC0 is called the RegressionResults instance will\n",
    "        then have another attribute `het_scale`, which is in this case is just\n",
    "        resid**2.\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.diag(self.cov_HC0))\n",
    "\n",
    "    @cache_readonly\n",
    "    def HC1_se(self):\n",
    "        \"\"\"\n",
    "        MacKinnon and White's (1985) heteroskedasticity robust standard errors.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Defined as sqrt(diag(n/(n-p)*HC_0).\n",
    "\n",
    "        When HC1_se or cov_HC1 is called the RegressionResults instance will\n",
    "        then have another attribute `het_scale`, which is in this case is\n",
    "        n/(n-p)*resid**2.\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.diag(self.cov_HC1))\n",
    "\n",
    "    @cache_readonly\n",
    "    def HC2_se(self):\n",
    "        \"\"\"\n",
    "        MacKinnon and White's (1985) heteroskedasticity robust standard errors.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Defined as (X.T X)^(-1)X.T diag(e_i^(2)/(1-h_ii)) X(X.T X)^(-1)\n",
    "        where h_ii = x_i(X.T X)^(-1)x_i.T\n",
    "\n",
    "        When HC2_se or cov_HC2 is called the RegressionResults instance will\n",
    "        then have another attribute `het_scale`, which is in this case is\n",
    "        resid^(2)/(1-h_ii).\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.diag(self.cov_HC2))\n",
    "\n",
    "    @cache_readonly\n",
    "    def HC3_se(self):\n",
    "        \"\"\"\n",
    "        MacKinnon and White's (1985) heteroskedasticity robust standard errors.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Defined as (X.T X)^(-1)X.T diag(e_i^(2)/(1-h_ii)^(2)) X(X.T X)^(-1)\n",
    "        where h_ii = x_i(X.T X)^(-1)x_i.T.\n",
    "\n",
    "        When HC3_se or cov_HC3 is called the RegressionResults instance will\n",
    "        then have another attribute `het_scale`, which is in this case is\n",
    "        resid^(2)/(1-h_ii)^(2).\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.diag(self.cov_HC3))\n",
    "\n",
    "    @cache_readonly\n",
    "    def resid_pearson(self):\n",
    "        \"\"\"\n",
    "        Residuals, normalized to have unit variance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The array `wresid` normalized by the sqrt of the scale to have\n",
    "            unit variance.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, 'resid'):\n",
    "            raise ValueError('Method requires residuals.')\n",
    "        eps = np.finfo(self.wresid.dtype).eps\n",
    "        if np.sqrt(self.scale) < 10 * eps * self.model.endog.mean():\n",
    "            # do not divide if scale is zero close to numerical precision\n",
    "            warnings.warn(\n",
    "                \"All residuals are 0, cannot compute normed residuals.\",\n",
    "                RuntimeWarning\n",
    "            )\n",
    "            return self.wresid\n",
    "        else:\n",
    "            return self.wresid / np.sqrt(self.scale)\n",
    "\n",
    "    def _is_nested(self, restricted):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        restricted : Result instance\n",
    "            The restricted model is assumed to be nested in the current\n",
    "            model. The result instance of the restricted model is required to\n",
    "            have two attributes, residual sum of squares, `ssr`, residual\n",
    "            degrees of freedom, `df_resid`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        nested : bool\n",
    "            True if nested, otherwise false\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        A most nests another model if the regressors in the smaller\n",
    "        model are spanned by the regressors in the larger model and\n",
    "        the regressand is identical.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.model.nobs != restricted.model.nobs:\n",
    "            return False\n",
    "\n",
    "        full_rank = self.model.rank\n",
    "        restricted_rank = restricted.model.rank\n",
    "        if full_rank <= restricted_rank:\n",
    "            return False\n",
    "\n",
    "        restricted_exog = restricted.model.wexog\n",
    "        full_wresid = self.wresid\n",
    "\n",
    "        scores = restricted_exog * full_wresid[:, None]\n",
    "        score_l2 = np.sqrt(np.mean(scores.mean(0) ** 2))\n",
    "        # TODO: Could be improved, and may fail depending on scale of\n",
    "        # regressors\n",
    "        return np.allclose(score_l2, 0)\n",
    "\n",
    "    def compare_lm_test(self, restricted, demean=True, use_lr=False):\n",
    "        \"\"\"\n",
    "        Use Lagrange Multiplier test to test a set of linear restrictions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        restricted : Result instance\n",
    "            The restricted model is assumed to be nested in the\n",
    "            current model. The result instance of the restricted model\n",
    "            is required to have two attributes, residual sum of\n",
    "            squares, `ssr`, residual degrees of freedom, `df_resid`.\n",
    "        demean : bool\n",
    "            Flag indicating whether the demean the scores based on the\n",
    "            residuals from the restricted model.  If True, the covariance of\n",
    "            the scores are used and the LM test is identical to the large\n",
    "            sample version of the LR test.\n",
    "        use_lr : bool\n",
    "            A flag indicating whether to estimate the covariance of the model\n",
    "            scores using the unrestricted model. Setting the to True improves\n",
    "            the power of the test.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lm_value : float\n",
    "            The test statistic which has a chi2 distributed.\n",
    "        p_value : float\n",
    "            The p-value of the test statistic.\n",
    "        df_diff : int\n",
    "            The degrees of freedom of the restriction, i.e. difference in df\n",
    "            between models.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The LM test examines whether the scores from the restricted model are\n",
    "        0. If the null is true, and the restrictions are valid, then the\n",
    "        parameters of the restricted model should be close to the minimum of\n",
    "        the sum of squared errors, and so the scores should be close to zero,\n",
    "        on average.\n",
    "        \"\"\"\n",
    "        from numpy.linalg import inv\n",
    "\n",
    "        import statsmodels.stats.sandwich_covariance as sw\n",
    "\n",
    "        if not self._is_nested(restricted):\n",
    "            raise ValueError(\"Restricted model is not nested by full model.\")\n",
    "\n",
    "        wresid = restricted.wresid\n",
    "        wexog = self.model.wexog\n",
    "        scores = wexog * wresid[:, None]\n",
    "\n",
    "        n = self.nobs\n",
    "        df_full = self.df_resid\n",
    "        df_restr = restricted.df_resid\n",
    "        df_diff = (df_restr - df_full)\n",
    "\n",
    "        s = scores.mean(axis=0)\n",
    "        if use_lr:\n",
    "            scores = wexog * self.wresid[:, None]\n",
    "            demean = False\n",
    "\n",
    "        if demean:\n",
    "            scores = scores - scores.mean(0)[None, :]\n",
    "        # Form matters here.  If homoskedastics can be sigma^2 (X'X)^-1\n",
    "        # If Heteroskedastic then the form below is fine\n",
    "        # If HAC then need to use HAC\n",
    "        # If Cluster, should use cluster\n",
    "\n",
    "        cov_type = getattr(self, 'cov_type', 'nonrobust')\n",
    "        if cov_type == 'nonrobust':\n",
    "            sigma2 = np.mean(wresid**2)\n",
    "            xpx = np.dot(wexog.T, wexog) / n\n",
    "            s_inv = inv(sigma2 * xpx)\n",
    "        elif cov_type in ('HC0', 'HC1', 'HC2', 'HC3'):\n",
    "            s_inv = inv(np.dot(scores.T, scores) / n)\n",
    "        elif cov_type == 'HAC':\n",
    "            maxlags = self.cov_kwds['maxlags']\n",
    "            s_inv = inv(sw.S_hac_simple(scores, maxlags) / n)\n",
    "        elif cov_type == 'cluster':\n",
    "            # cluster robust standard errors\n",
    "            groups = self.cov_kwds['groups']\n",
    "            # TODO: Might need demean option in S_crosssection by group?\n",
    "            s_inv = inv(sw.S_crosssection(scores, groups))\n",
    "        else:\n",
    "            raise ValueError('Only nonrobust, HC, HAC and cluster are ' +\n",
    "                             'currently connected')\n",
    "\n",
    "        lm_value = n * (s @ s_inv @ s.T)\n",
    "        p_value = stats.chi2.sf(lm_value, df_diff)\n",
    "        return lm_value, p_value, df_diff\n",
    "\n",
    "    def compare_f_test(self, restricted):\n",
    "        \"\"\"\n",
    "        Use F test to test whether restricted model is correct.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        restricted : Result instance\n",
    "            The restricted model is assumed to be nested in the\n",
    "            current model. The result instance of the restricted model\n",
    "            is required to have two attributes, residual sum of\n",
    "            squares, `ssr`, residual degrees of freedom, `df_resid`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        f_value : float\n",
    "            The test statistic which has an F distribution.\n",
    "        p_value : float\n",
    "            The p-value of the test statistic.\n",
    "        df_diff : int\n",
    "            The degrees of freedom of the restriction, i.e. difference in\n",
    "            df between models.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        See mailing list discussion October 17,\n",
    "\n",
    "        This test compares the residual sum of squares of the two\n",
    "        models.  This is not a valid test, if there is unspecified\n",
    "        heteroscedasticity or correlation. This method will issue a\n",
    "        warning if this is detected but still return the results under\n",
    "        the assumption of homoscedasticity and no autocorrelation\n",
    "        (sphericity).\n",
    "        \"\"\"\n",
    "\n",
    "        has_robust1 = getattr(self, 'cov_type', 'nonrobust') != 'nonrobust'\n",
    "        has_robust2 = (getattr(restricted, 'cov_type', 'nonrobust') !=\n",
    "                       'nonrobust')\n",
    "\n",
    "        if has_robust1 or has_robust2:\n",
    "            warnings.warn('F test for comparison is likely invalid with ' +\n",
    "                          'robust covariance, proceeding anyway',\n",
    "                          InvalidTestWarning)\n",
    "\n",
    "        ssr_full = self.ssr\n",
    "        ssr_restr = restricted.ssr\n",
    "        df_full = self.df_resid\n",
    "        df_restr = restricted.df_resid\n",
    "\n",
    "        df_diff = (df_restr - df_full)\n",
    "        f_value = (ssr_restr - ssr_full) / df_diff / ssr_full * df_full\n",
    "        p_value = stats.f.sf(f_value, df_diff, df_full)\n",
    "        return f_value, p_value, df_diff\n",
    "\n",
    "    def compare_lr_test(self, restricted, large_sample=False):\n",
    "        \"\"\"\n",
    "        Likelihood ratio test to test whether restricted model is correct.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        restricted : Result instance\n",
    "            The restricted model is assumed to be nested in the current model.\n",
    "            The result instance of the restricted model is required to have two\n",
    "            attributes, residual sum of squares, `ssr`, residual degrees of\n",
    "            freedom, `df_resid`.\n",
    "\n",
    "        large_sample : bool\n",
    "            Flag indicating whether to use a heteroskedasticity robust version\n",
    "            of the LR test, which is a modified LM test.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lr_stat : float\n",
    "            The likelihood ratio which is chisquare distributed with df_diff\n",
    "            degrees of freedom.\n",
    "        p_value : float\n",
    "            The p-value of the test statistic.\n",
    "        df_diff : int\n",
    "            The degrees of freedom of the restriction, i.e. difference in df\n",
    "            between models.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The exact likelihood ratio is valid for homoskedastic data,\n",
    "        and is defined as\n",
    "\n",
    "        .. math:: D=-2\\\\log\\\\left(\\\\frac{\\\\mathcal{L}_{null}}\n",
    "           {\\\\mathcal{L}_{alternative}}\\\\right)\n",
    "\n",
    "        where :math:`\\\\mathcal{L}` is the likelihood of the\n",
    "        model. With :math:`D` distributed as chisquare with df equal\n",
    "        to difference in number of parameters or equivalently\n",
    "        difference in residual degrees of freedom.\n",
    "\n",
    "        The large sample version of the likelihood ratio is defined as\n",
    "\n",
    "        .. math:: D=n s^{\\\\prime}S^{-1}s\n",
    "\n",
    "        where :math:`s=n^{-1}\\\\sum_{i=1}^{n} s_{i}`\n",
    "\n",
    "        .. math:: s_{i} = x_{i,alternative} \\\\epsilon_{i,null}\n",
    "\n",
    "        is the average score of the model evaluated using the\n",
    "        residuals from null model and the regressors from the\n",
    "        alternative model and :math:`S` is the covariance of the\n",
    "        scores, :math:`s_{i}`.  The covariance of the scores is\n",
    "        estimated using the same estimator as in the alternative\n",
    "        model.\n",
    "\n",
    "        This test compares the loglikelihood of the two models.  This\n",
    "        may not be a valid test, if there is unspecified\n",
    "        heteroscedasticity or correlation. This method will issue a\n",
    "        warning if this is detected but still return the results\n",
    "        without taking unspecified heteroscedasticity or correlation\n",
    "        into account.\n",
    "\n",
    "        This test compares the loglikelihood of the two models.  This\n",
    "        may not be a valid test, if there is unspecified\n",
    "        heteroscedasticity or correlation. This method will issue a\n",
    "        warning if this is detected but still return the results\n",
    "        without taking unspecified heteroscedasticity or correlation\n",
    "        into account.\n",
    "\n",
    "        is the average score of the model evaluated using the\n",
    "        residuals from null model and the regressors from the\n",
    "        alternative model and :math:`S` is the covariance of the\n",
    "        scores, :math:`s_{i}`.  The covariance of the scores is\n",
    "        estimated using the same estimator as in the alternative\n",
    "        model.\n",
    "        \"\"\"\n",
    "        # TODO: put into separate function, needs tests\n",
    "\n",
    "        # See mailing list discussion October 17,\n",
    "\n",
    "        if large_sample:\n",
    "            return self.compare_lm_test(restricted, use_lr=True)\n",
    "\n",
    "        has_robust1 = (getattr(self, 'cov_type', 'nonrobust') != 'nonrobust')\n",
    "        has_robust2 = (\n",
    "            getattr(restricted, 'cov_type', 'nonrobust') != 'nonrobust')\n",
    "\n",
    "        if has_robust1 or has_robust2:\n",
    "            warnings.warn('Likelihood Ratio test is likely invalid with ' +\n",
    "                          'robust covariance, proceeding anyway',\n",
    "                          InvalidTestWarning)\n",
    "\n",
    "        llf_full = self.llf\n",
    "        llf_restr = restricted.llf\n",
    "        df_full = self.df_resid\n",
    "        df_restr = restricted.df_resid\n",
    "\n",
    "        lrdf = (df_restr - df_full)\n",
    "        lrstat = -2*(llf_restr - llf_full)\n",
    "        lr_pvalue = stats.chi2.sf(lrstat, lrdf)\n",
    "\n",
    "        return lrstat, lr_pvalue, lrdf\n",
    "\n",
    "    def get_robustcov_results(self, cov_type='HC1', use_t=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Create new results instance with robust covariance as default.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        cov_type : str\n",
    "            The type of robust sandwich estimator to use. See Notes below.\n",
    "        use_t : bool\n",
    "            If true, then the t distribution is used for inference.\n",
    "            If false, then the normal distribution is used.\n",
    "            If `use_t` is None, then an appropriate default is used, which is\n",
    "            `True` if the cov_type is nonrobust, and `False` in all other\n",
    "            cases.\n",
    "        **kwargs\n",
    "            Required or optional arguments for robust covariance calculation.\n",
    "            See Notes below.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        RegressionResults\n",
    "            This method creates a new results instance with the\n",
    "            requested robust covariance as the default covariance of\n",
    "            the parameters.  Inferential statistics like p-values and\n",
    "            hypothesis tests will be based on this covariance matrix.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The following covariance types and required or optional arguments are\n",
    "        currently available:\n",
    "\n",
    "        - 'fixed scale' uses a predefined scale\n",
    "\n",
    "          ``scale``: float, optional\n",
    "            Argument to set the scale. Default is 1.\n",
    "\n",
    "        - 'HC0', 'HC1', 'HC2', 'HC3': heteroscedasticity robust covariance\n",
    "\n",
    "          - no keyword arguments\n",
    "\n",
    "        - 'HAC': heteroskedasticity-autocorrelation robust covariance\n",
    "\n",
    "          ``maxlags`` :  integer, required\n",
    "            number of lags to use\n",
    "\n",
    "          ``kernel`` : {callable, str}, optional\n",
    "            kernels currently available kernels are ['bartlett', 'uniform'],\n",
    "            default is Bartlett\n",
    "\n",
    "          ``use_correction``: bool, optional\n",
    "            If true, use small sample correction\n",
    "\n",
    "        - 'cluster': clustered covariance estimator\n",
    "\n",
    "          ``groups`` : array_like[int], required :\n",
    "            Integer-valued index of clusters or groups.\n",
    "\n",
    "          ``use_correction``: bool, optional\n",
    "            If True the sandwich covariance is calculated with a small\n",
    "            sample correction.\n",
    "            If False the sandwich covariance is calculated without\n",
    "            small sample correction.\n",
    "\n",
    "          ``df_correction``: bool, optional\n",
    "            If True (default), then the degrees of freedom for the\n",
    "            inferential statistics and hypothesis tests, such as\n",
    "            pvalues, f_pvalue, conf_int, and t_test and f_test, are\n",
    "            based on the number of groups minus one instead of the\n",
    "            total number of observations minus the number of explanatory\n",
    "            variables. `df_resid` of the results instance is also\n",
    "            adjusted. When `use_t` is also True, then pvalues are\n",
    "            computed using the Student's t distribution using the\n",
    "            corrected values. These may differ substantially from\n",
    "            p-values based on the normal is the number of groups is\n",
    "            small.\n",
    "            If False, then `df_resid` of the results instance is not\n",
    "            adjusted.\n",
    "\n",
    "        - 'hac-groupsum': Driscoll and Kraay, heteroscedasticity and\n",
    "          autocorrelation robust covariance for panel data\n",
    "          # TODO: more options needed here\n",
    "\n",
    "          ``time`` : array_like, required\n",
    "            index of time periods\n",
    "          ``maxlags`` : integer, required\n",
    "            number of lags to use\n",
    "          ``kernel`` : {callable, str}, optional\n",
    "            The available kernels are ['bartlett', 'uniform']. The default is\n",
    "            Bartlett.\n",
    "          ``use_correction`` : {False, 'hac', 'cluster'}, optional\n",
    "            If False the the sandwich covariance is calculated without small\n",
    "            sample correction. If `use_correction = 'cluster'` (default),\n",
    "            then the same small sample correction as in the case of\n",
    "            `covtype='cluster'` is used.\n",
    "          ``df_correction`` : bool, optional\n",
    "            The adjustment to df_resid, see cov_type 'cluster' above\n",
    "\n",
    "        - 'hac-panel': heteroscedasticity and autocorrelation robust standard\n",
    "          errors in panel data. The data needs to be sorted in this case, the\n",
    "          time series for each panel unit or cluster need to be stacked. The\n",
    "          membership to a time series of an individual or group can be either\n",
    "          specified by group indicators or by increasing time periods. One of\n",
    "          ``groups`` or ``time`` is required. # TODO: we need more options here\n",
    "\n",
    "          ``groups`` : array_like[int]\n",
    "            indicator for groups\n",
    "          ``time`` : array_like[int]\n",
    "            index of time periods\n",
    "          ``maxlags`` : int, required\n",
    "            number of lags to use\n",
    "          ``kernel`` : {callable, str}, optional\n",
    "            Available kernels are ['bartlett', 'uniform'], default\n",
    "            is Bartlett\n",
    "          ``use_correction`` : {False, 'hac', 'cluster'}, optional\n",
    "            If False the sandwich covariance is calculated without\n",
    "            small sample correction.\n",
    "          ``df_correction`` : bool, optional\n",
    "            Adjustment to df_resid, see cov_type 'cluster' above\n",
    "\n",
    "        **Reminder**: ``use_correction`` in \"hac-groupsum\" and \"hac-panel\" is\n",
    "        not bool, needs to be in {False, 'hac', 'cluster'}.\n",
    "\n",
    "        .. todo:: Currently there is no check for extra or misspelled keywords,\n",
    "             except in the case of cov_type `HCx`\n",
    "        \"\"\"\n",
    "        from statsmodels.base.covtype import descriptions, normalize_cov_type\n",
    "        import statsmodels.stats.sandwich_covariance as sw\n",
    "\n",
    "        cov_type = normalize_cov_type(cov_type)\n",
    "\n",
    "        if 'kernel' in kwargs:\n",
    "            kwargs['weights_func'] = kwargs.pop('kernel')\n",
    "        if 'weights_func' in kwargs and not callable(kwargs['weights_func']):\n",
    "            kwargs['weights_func'] = sw.kernel_dict[kwargs['weights_func']]\n",
    "\n",
    "        # TODO: make separate function that returns a robust cov plus info\n",
    "        use_self = kwargs.pop('use_self', False)\n",
    "        if use_self:\n",
    "            res = self\n",
    "        else:\n",
    "            res = self.__class__(\n",
    "                self.model, self.params,\n",
    "                normalized_cov_params=self.normalized_cov_params,\n",
    "                scale=self.scale)\n",
    "\n",
    "        res.cov_type = cov_type\n",
    "        # use_t might already be defined by the class, and already set\n",
    "        if use_t is None:\n",
    "            use_t = self.use_t\n",
    "        res.cov_kwds = {'use_t': use_t}  # store for information\n",
    "        res.use_t = use_t\n",
    "\n",
    "        adjust_df = False\n",
    "        if cov_type in ['cluster', 'hac-panel', 'hac-groupsum']:\n",
    "            df_correction = kwargs.get('df_correction', None)\n",
    "            # TODO: check also use_correction, do I need all combinations?\n",
    "            if df_correction is not False:  # i.e. in [None, True]:\n",
    "                # user did not explicitely set it to False\n",
    "                adjust_df = True\n",
    "\n",
    "        res.cov_kwds['adjust_df'] = adjust_df\n",
    "\n",
    "        # verify and set kwargs, and calculate cov\n",
    "        # TODO: this should be outsourced in a function so we can reuse it in\n",
    "        #       other models\n",
    "        # TODO: make it DRYer   repeated code for checking kwargs\n",
    "        if cov_type in ['fixed scale', 'fixed_scale']:\n",
    "            res.cov_kwds['description'] = descriptions['fixed_scale']\n",
    "\n",
    "            res.cov_kwds['scale'] = scale = kwargs.get('scale', 1.)\n",
    "            res.cov_params_default = scale * res.normalized_cov_params\n",
    "        elif cov_type.upper() in ('HC0', 'HC1', 'HC2', 'HC3'):\n",
    "            if kwargs:\n",
    "                raise ValueError('heteroscedasticity robust covariance '\n",
    "                                 'does not use keywords')\n",
    "            res.cov_kwds['description'] = descriptions[cov_type.upper()]\n",
    "            res.cov_params_default = getattr(self, 'cov_' + cov_type.upper())\n",
    "        elif cov_type.lower() == 'hac':\n",
    "            # TODO: check if required, default in cov_hac_simple\n",
    "            maxlags = kwargs['maxlags']\n",
    "            res.cov_kwds['maxlags'] = maxlags\n",
    "            weights_func = kwargs.get('weights_func', sw.weights_bartlett)\n",
    "            res.cov_kwds['weights_func'] = weights_func\n",
    "            use_correction = kwargs.get('use_correction', False)\n",
    "            res.cov_kwds['use_correction'] = use_correction\n",
    "            res.cov_kwds['description'] = descriptions['HAC'].format(\n",
    "                maxlags=maxlags,\n",
    "                correction=['without', 'with'][use_correction])\n",
    "\n",
    "            res.cov_params_default = sw.cov_hac_simple(\n",
    "                self, nlags=maxlags, weights_func=weights_func,\n",
    "                use_correction=use_correction)\n",
    "        elif cov_type.lower() == 'cluster':\n",
    "            # cluster robust standard errors, one- or two-way\n",
    "            groups = kwargs['groups']\n",
    "            if not hasattr(groups, 'shape'):\n",
    "                groups = [np.squeeze(np.asarray(group)) for group in groups]\n",
    "                groups = np.asarray(groups).T\n",
    "\n",
    "            if groups.ndim >= 2:\n",
    "                groups = groups.squeeze()\n",
    "\n",
    "            res.cov_kwds['groups'] = groups\n",
    "            use_correction = kwargs.get('use_correction', True)\n",
    "            res.cov_kwds['use_correction'] = use_correction\n",
    "            if groups.ndim == 1:\n",
    "                if adjust_df:\n",
    "                    # need to find number of groups\n",
    "                    # duplicate work\n",
    "                    self.n_groups = n_groups = len(np.unique(groups))\n",
    "                res.cov_params_default = sw.cov_cluster(\n",
    "                    self, groups, use_correction=use_correction)\n",
    "\n",
    "            elif groups.ndim == 2:\n",
    "                if hasattr(groups, 'values'):\n",
    "                    groups = groups.values\n",
    "\n",
    "                if adjust_df:\n",
    "                    # need to find number of groups\n",
    "                    # duplicate work\n",
    "                    n_groups0 = len(np.unique(groups[:, 0]))\n",
    "                    n_groups1 = len(np.unique(groups[:, 1]))\n",
    "                    self.n_groups = (n_groups0, n_groups1)\n",
    "                    n_groups = min(n_groups0, n_groups1)  # use for adjust_df\n",
    "\n",
    "                # Note: sw.cov_cluster_2groups has 3 returns\n",
    "                res.cov_params_default = sw.cov_cluster_2groups(\n",
    "                    self, groups, use_correction=use_correction)[0]\n",
    "            else:\n",
    "                raise ValueError('only two groups are supported')\n",
    "            res.cov_kwds['description'] = descriptions['cluster']\n",
    "\n",
    "        elif cov_type.lower() == 'hac-panel':\n",
    "            # cluster robust standard errors\n",
    "            res.cov_kwds['time'] = time = kwargs.get('time', None)\n",
    "            res.cov_kwds['groups'] = groups = kwargs.get('groups', None)\n",
    "            # TODO: nlags is currently required\n",
    "            # nlags = kwargs.get('nlags', True)\n",
    "            # res.cov_kwds['nlags'] = nlags\n",
    "            # TODO: `nlags` or `maxlags`\n",
    "            res.cov_kwds['maxlags'] = maxlags = kwargs['maxlags']\n",
    "            use_correction = kwargs.get('use_correction', 'hac')\n",
    "            res.cov_kwds['use_correction'] = use_correction\n",
    "            weights_func = kwargs.get('weights_func', sw.weights_bartlett)\n",
    "            res.cov_kwds['weights_func'] = weights_func\n",
    "            if groups is not None:\n",
    "                groups = np.asarray(groups)\n",
    "                tt = (np.nonzero(groups[:-1] != groups[1:])[0] + 1).tolist()\n",
    "                nobs_ = len(groups)\n",
    "            elif time is not None:\n",
    "                time = np.asarray(time)\n",
    "                # TODO: clumsy time index in cov_nw_panel\n",
    "                tt = (np.nonzero(time[1:] < time[:-1])[0] + 1).tolist()\n",
    "                nobs_ = len(time)\n",
    "            else:\n",
    "                raise ValueError('either time or groups needs to be given')\n",
    "            groupidx = lzip([0] + tt, tt + [nobs_])\n",
    "            self.n_groups = n_groups = len(groupidx)\n",
    "            res.cov_params_default = sw.cov_nw_panel(\n",
    "                self,\n",
    "                maxlags,\n",
    "                groupidx,\n",
    "                weights_func=weights_func,\n",
    "                use_correction=use_correction\n",
    "            )\n",
    "            res.cov_kwds['description'] = descriptions['HAC-Panel']\n",
    "\n",
    "        elif cov_type.lower() == 'hac-groupsum':\n",
    "            # Driscoll-Kraay standard errors\n",
    "            res.cov_kwds['time'] = time = kwargs['time']\n",
    "            # TODO: nlags is currently required\n",
    "            # nlags = kwargs.get('nlags', True)\n",
    "            # res.cov_kwds['nlags'] = nlags\n",
    "            # TODO: `nlags` or `maxlags`\n",
    "            res.cov_kwds['maxlags'] = maxlags = kwargs['maxlags']\n",
    "            use_correction = kwargs.get('use_correction', 'cluster')\n",
    "            res.cov_kwds['use_correction'] = use_correction\n",
    "            weights_func = kwargs.get('weights_func', sw.weights_bartlett)\n",
    "            res.cov_kwds['weights_func'] = weights_func\n",
    "            if adjust_df:\n",
    "                # need to find number of groups\n",
    "                tt = (np.nonzero(time[1:] < time[:-1])[0] + 1)\n",
    "                self.n_groups = n_groups = len(tt) + 1\n",
    "            res.cov_params_default = sw.cov_nw_groupsum(\n",
    "                self, maxlags, time, weights_func=weights_func,\n",
    "                use_correction=use_correction)\n",
    "            res.cov_kwds['description'] = descriptions['HAC-Groupsum']\n",
    "        else:\n",
    "            raise ValueError('cov_type not recognized. See docstring for ' +\n",
    "                             'available options and spelling')\n",
    "\n",
    "        if adjust_df:\n",
    "            # Note: df_resid is used for scale and others, add new attribute\n",
    "            res.df_resid_inference = n_groups - 1\n",
    "\n",
    "        return res\n",
    "\n",
    "    @Appender(pred.get_prediction.__doc__)\n",
    "    def get_prediction(self, exog=None, transform=True, weights=None,\n",
    "                       row_labels=None, **kwargs):\n",
    "\n",
    "        return pred.get_prediction(\n",
    "            self, exog=exog, transform=transform, weights=weights,\n",
    "            row_labels=row_labels, **kwargs)\n",
    "\n",
    "    def summary(\n",
    "            self,\n",
    "            yname: str | None = None,\n",
    "            xname: Sequence[str] | None = None,\n",
    "            title: str | None = None,\n",
    "            alpha: float = 0.05,\n",
    "            slim: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Summarize the Regression Results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        yname : str, optional\n",
    "            Name of endogenous (response) variable. The Default is `y`.\n",
    "        xname : list[str], optional\n",
    "            Names for the exogenous variables. Default is `var_##` for ## in\n",
    "            the number of regressors. Must match the number of parameters\n",
    "            in the model.\n",
    "        title : str, optional\n",
    "            Title for the top table. If not None, then this replaces the\n",
    "            default title.\n",
    "        alpha : float, optional\n",
    "            The significance level for the confidence intervals.\n",
    "        slim : bool, optional\n",
    "            Flag indicating to produce reduced set or diagnostic information.\n",
    "            Default is False.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Summary\n",
    "            Instance holding the summary tables and text, which can be printed\n",
    "            or converted to various output formats.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.iolib.summary.Summary : A class that holds summary results.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.stattools import (\n",
    "            durbin_watson,\n",
    "            jarque_bera,\n",
    "            omni_normtest,\n",
    "        )\n",
    "        alpha = float_like(alpha, \"alpha\", optional=False)\n",
    "        slim = bool_like(slim, \"slim\", optional=False, strict=True)\n",
    "\n",
    "        jb, jbpv, skew, kurtosis = jarque_bera(self.wresid)\n",
    "        omni, omnipv = omni_normtest(self.wresid)\n",
    "\n",
    "        eigvals = self.eigenvals\n",
    "        condno = self.condition_number\n",
    "\n",
    "        # TODO: Avoid adding attributes in non-__init__\n",
    "        self.diagn = dict(jb=jb, jbpv=jbpv, skew=skew, kurtosis=kurtosis,\n",
    "                          omni=omni, omnipv=omnipv, condno=condno,\n",
    "                          mineigval=eigvals[-1])\n",
    "\n",
    "        # TODO not used yet\n",
    "        # diagn_left_header = ['Models stats']\n",
    "        # diagn_right_header = ['Residual stats']\n",
    "\n",
    "        # TODO: requiring list/iterable is a bit annoying\n",
    "        #   need more control over formatting\n",
    "        # TODO: default do not work if it's not identically spelled\n",
    "\n",
    "        top_left = [('Dep. Variable:', None),\n",
    "                    ('Model:', None),\n",
    "                    ('Method:', ['Least Squares']),\n",
    "                    ('Date:', None),\n",
    "                    ('Time:', None),\n",
    "                    ('No. Observations:', None),\n",
    "                    ('Df Residuals:', None),\n",
    "                    ('Df Model:', None),\n",
    "                    ]\n",
    "\n",
    "        if hasattr(self, 'cov_type'):\n",
    "            top_left.append(('Covariance Type:', [self.cov_type]))\n",
    "\n",
    "        rsquared_type = '' if self.k_constant else ' (uncentered)'\n",
    "        top_right = [('R-squared' + rsquared_type + ':',\n",
    "                      [\"%#8.3f\" % self.rsquared]),\n",
    "                     ('Adj. R-squared' + rsquared_type + ':',\n",
    "                      [\"%#8.3f\" % self.rsquared_adj]),\n",
    "                     ('F-statistic:', [\"%#8.4g\" % self.fvalue]),\n",
    "                     ('Prob (F-statistic):', [\"%#6.3g\" % self.f_pvalue]),\n",
    "                     ('Log-Likelihood:', None),\n",
    "                     ('AIC:', [\"%#8.4g\" % self.aic]),\n",
    "                     ('BIC:', [\"%#8.4g\" % self.bic])\n",
    "                     ]\n",
    "\n",
    "        if slim:\n",
    "            slimlist = ['Dep. Variable:', 'Model:', 'No. Observations:',\n",
    "                        'Covariance Type:', 'R-squared:', 'Adj. R-squared:',\n",
    "                        'F-statistic:', 'Prob (F-statistic):']\n",
    "            diagn_left = diagn_right = []\n",
    "            top_left = [elem for elem in top_left if elem[0] in slimlist]\n",
    "            top_right = [elem for elem in top_right if elem[0] in slimlist]\n",
    "            top_right = top_right + \\\n",
    "                [(\"\", [])] * (len(top_left) - len(top_right))\n",
    "        else:\n",
    "            diagn_left = [('Omnibus:', [\"%#6.3f\" % omni]),\n",
    "                          ('Prob(Omnibus):', [\"%#6.3f\" % omnipv]),\n",
    "                          ('Skew:', [\"%#6.3f\" % skew]),\n",
    "                          ('Kurtosis:', [\"%#6.3f\" % kurtosis])\n",
    "                          ]\n",
    "\n",
    "            diagn_right = [('Durbin-Watson:',\n",
    "                            [\"%#8.3f\" % durbin_watson(self.wresid)]\n",
    "                            ),\n",
    "                           ('Jarque-Bera (JB):', [\"%#8.3f\" % jb]),\n",
    "                           ('Prob(JB):', [\"%#8.3g\" % jbpv]),\n",
    "                           ('Cond. No.', [\"%#8.3g\" % condno])\n",
    "                           ]\n",
    "\n",
    "        if title is None:\n",
    "            title = self.model.__class__.__name__ + ' ' + \"Regression Results\"\n",
    "\n",
    "        # create summary table instance\n",
    "        from statsmodels.iolib.summary import Summary\n",
    "        smry = Summary()\n",
    "        smry.add_table_2cols(self, gleft=top_left, gright=top_right,\n",
    "                             yname=yname, xname=xname, title=title)\n",
    "        smry.add_table_params(self, yname=yname, xname=xname, alpha=alpha,\n",
    "                              use_t=self.use_t)\n",
    "        if not slim:\n",
    "            smry.add_table_2cols(self, gleft=diagn_left, gright=diagn_right,\n",
    "                                 yname=yname, xname=xname,\n",
    "                                 title=\"\")\n",
    "\n",
    "        # add warnings/notes, added to text format only\n",
    "        etext = []\n",
    "        if not self.k_constant:\n",
    "            etext.append(\n",
    "                \"R is computed without centering (uncentered) since the \"\n",
    "                \"model does not contain a constant.\"\n",
    "            )\n",
    "        if hasattr(self, 'cov_type'):\n",
    "            etext.append(self.cov_kwds['description'])\n",
    "        if self.model.exog.shape[0] < self.model.exog.shape[1]:\n",
    "            wstr = \"The input rank is higher than the number of observations.\"\n",
    "            etext.append(wstr)\n",
    "        if eigvals[-1] < 1e-10:\n",
    "            wstr = \"The smallest eigenvalue is %6.3g. This might indicate \"\n",
    "            wstr += \"that there are\\n\"\n",
    "            wstr += \"strong multicollinearity problems or that the design \"\n",
    "            wstr += \"matrix is singular.\"\n",
    "            wstr = wstr % eigvals[-1]\n",
    "            etext.append(wstr)\n",
    "        elif condno > 1000:  # TODO: what is recommended?\n",
    "            wstr = \"The condition number is large, %6.3g. This might \"\n",
    "            wstr += \"indicate that there are\\n\"\n",
    "            wstr += \"strong multicollinearity or other numerical \"\n",
    "            wstr += \"problems.\"\n",
    "            wstr = wstr % condno\n",
    "            etext.append(wstr)\n",
    "\n",
    "        if etext:\n",
    "            etext = [f\"[{i + 1}] {text}\"\n",
    "                     for i, text in enumerate(etext)]\n",
    "            etext.insert(0, \"Notes:\")\n",
    "            smry.add_extra_txt(etext)\n",
    "\n",
    "        return smry\n",
    "\n",
    "    def summary2(\n",
    "            self,\n",
    "            yname: str | None = None,\n",
    "            xname: Sequence[str] | None = None,\n",
    "            title: str | None = None,\n",
    "            alpha: float = 0.05,\n",
    "            float_format: str = \"%.4f\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Experimental summary function to summarize the regression results.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        yname : str\n",
    "            The name of the dependent variable (optional).\n",
    "        xname : list[str], optional\n",
    "            Names for the exogenous variables. Default is `var_##` for ## in\n",
    "            the number of regressors. Must match the number of parameters\n",
    "            in the model.\n",
    "        title : str, optional\n",
    "            Title for the top table. If not None, then this replaces the\n",
    "            default title.\n",
    "        alpha : float\n",
    "            The significance level for the confidence intervals.\n",
    "        float_format : str\n",
    "            The format for floats in parameters summary.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Summary\n",
    "            Instance holding the summary tables and text, which can be printed\n",
    "            or converted to various output formats.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.iolib.summary2.Summary\n",
    "            A class that holds summary results.\n",
    "        \"\"\"\n",
    "        # Diagnostics\n",
    "        from statsmodels.stats.stattools import (\n",
    "            durbin_watson,\n",
    "            jarque_bera,\n",
    "            omni_normtest,\n",
    "        )\n",
    "\n",
    "        jb, jbpv, skew, kurtosis = jarque_bera(self.wresid)\n",
    "        omni, omnipv = omni_normtest(self.wresid)\n",
    "        dw = durbin_watson(self.wresid)\n",
    "        eigvals = self.eigenvals\n",
    "        condno = self.condition_number\n",
    "        diagnostic = dict([\n",
    "            ('Omnibus:',  \"%.3f\" % omni),\n",
    "            ('Prob(Omnibus):', \"%.3f\" % omnipv),\n",
    "            ('Skew:', \"%.3f\" % skew),\n",
    "            ('Kurtosis:', \"%.3f\" % kurtosis),\n",
    "            ('Durbin-Watson:', \"%.3f\" % dw),\n",
    "            ('Jarque-Bera (JB):', \"%.3f\" % jb),\n",
    "            ('Prob(JB):', \"%.3f\" % jbpv),\n",
    "            ('Condition No.:', \"%.0f\" % condno)\n",
    "            ])\n",
    "\n",
    "        # Summary\n",
    "        from statsmodels.iolib import summary2\n",
    "        smry = summary2.Summary()\n",
    "        smry.add_base(results=self, alpha=alpha, float_format=float_format,\n",
    "                      xname=xname, yname=yname, title=title)\n",
    "        smry.add_dict(diagnostic)\n",
    "\n",
    "        etext = []\n",
    "\n",
    "        if not self.k_constant:\n",
    "            etext.append(\n",
    "                \"R is computed without centering (uncentered) since the \\\n",
    "                model does not contain a constant.\"\n",
    "            )\n",
    "        if hasattr(self, 'cov_type'):\n",
    "            etext.append(self.cov_kwds['description'])\n",
    "        if self.model.exog.shape[0] < self.model.exog.shape[1]:\n",
    "            wstr = \"The input rank is higher than the number of observations.\"\n",
    "            etext.append(wstr)\n",
    "\n",
    "        # Warnings\n",
    "        if eigvals[-1] < 1e-10:\n",
    "            warn = \"The smallest eigenvalue is %6.3g. This might indicate that\\\n",
    "                there are strong multicollinearity problems or that the design\\\n",
    "                matrix is singular.\" % eigvals[-1]\n",
    "            etext.append(warn)\n",
    "        elif condno > 1000:\n",
    "            warn = \"The condition number is large, %6.3g. This might indicate\\\n",
    "                that there are strong multicollinearity or other numerical\\\n",
    "                problems.\" % condno\n",
    "            etext.append(warn)\n",
    "\n",
    "        if etext:\n",
    "            etext = [f\"[{i + 1}] {text}\"\n",
    "                     for i, text in enumerate(etext)]\n",
    "            etext.insert(0, \"Notes:\")\n",
    "\n",
    "        for line in etext:\n",
    "            smry.add_text(line)\n",
    "\n",
    "        return smry\n",
    "\n",
    "\n",
    "class OLSResults(RegressionResults):\n",
    "    \"\"\"\n",
    "    Results class for for an OLS model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RegressionModel\n",
    "        The regression model instance.\n",
    "    params : ndarray\n",
    "        The estimated parameters.\n",
    "    normalized_cov_params : ndarray\n",
    "        The normalized covariance parameters.\n",
    "    scale : float\n",
    "        The estimated scale of the residuals.\n",
    "    cov_type : str\n",
    "        The covariance estimator used in the results.\n",
    "    cov_kwds : dict\n",
    "        Additional keywords used in the covariance specification.\n",
    "    use_t : bool\n",
    "        Flag indicating to use the Student's t in inference.\n",
    "    **kwargs\n",
    "        Additional keyword arguments used to initialize the results.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    RegressionResults\n",
    "        Results store for WLS and GLW models.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Most of the methods and attributes are inherited from RegressionResults.\n",
    "    The special methods that are only available for OLS are:\n",
    "\n",
    "    - get_influence\n",
    "    - outlier_test\n",
    "    - el_test\n",
    "    - conf_int_el\n",
    "    \"\"\"\n",
    "\n",
    "    def get_influence(self):\n",
    "        \"\"\"\n",
    "        Calculate influence and outlier measures.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        OLSInfluence\n",
    "            The instance containing methods to calculate the main influence and\n",
    "            outlier measures for the OLS regression.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        statsmodels.stats.outliers_influence.OLSInfluence\n",
    "            A class that exposes methods to examine observation influence.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import OLSInfluence\n",
    "        return OLSInfluence(self)\n",
    "\n",
    "    def outlier_test(self, method='bonf', alpha=.05, labels=None,\n",
    "                     order=False, cutoff=None):\n",
    "        \"\"\"\n",
    "        Test observations for outliers according to method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str\n",
    "            The method to use in the outlier test.  Must be one of:\n",
    "\n",
    "            - `bonferroni` : one-step correction\n",
    "            - `sidak` : one-step correction\n",
    "            - `holm-sidak` :\n",
    "            - `holm` :\n",
    "            - `simes-hochberg` :\n",
    "            - `hommel` :\n",
    "            - `fdr_bh` : Benjamini/Hochberg\n",
    "            - `fdr_by` : Benjamini/Yekutieli\n",
    "\n",
    "            See `statsmodels.stats.multitest.multipletests` for details.\n",
    "        alpha : float\n",
    "            The familywise error rate (FWER).\n",
    "        labels : None or array_like\n",
    "            If `labels` is not None, then it will be used as index to the\n",
    "            returned pandas DataFrame. See also Returns below.\n",
    "        order : bool\n",
    "            Whether or not to order the results by the absolute value of the\n",
    "            studentized residuals. If labels are provided they will also be\n",
    "            sorted.\n",
    "        cutoff : None or float in [0, 1]\n",
    "            If cutoff is not None, then the return only includes observations\n",
    "            with multiple testing corrected p-values strictly below the cutoff.\n",
    "            The returned array or dataframe can be empty if t.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            Returns either an ndarray or a DataFrame if labels is not None.\n",
    "            Will attempt to get labels from model_results if available. The\n",
    "            columns are the Studentized residuals, the unadjusted p-value,\n",
    "            and the corrected p-value according to method.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The unadjusted p-value is stats.t.sf(abs(resid), df) where\n",
    "        df = df_resid - 1.\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.outliers_influence import outlier_test\n",
    "        return outlier_test(self, method, alpha, labels=labels,\n",
    "                            order=order, cutoff=cutoff)\n",
    "\n",
    "    def el_test(self, b0_vals, param_nums, return_weights=0, ret_params=0,\n",
    "                method='nm', stochastic_exog=1):\n",
    "        \"\"\"\n",
    "        Test single or joint hypotheses using Empirical Likelihood.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        b0_vals : 1darray\n",
    "            The hypothesized value of the parameter to be tested.\n",
    "        param_nums : 1darray\n",
    "            The parameter number to be tested.\n",
    "        return_weights : bool\n",
    "            If true, returns the weights that optimize the likelihood\n",
    "            ratio at b0_vals. The default is False.\n",
    "        ret_params : bool\n",
    "            If true, returns the parameter vector that maximizes the likelihood\n",
    "            ratio at b0_vals.  Also returns the weights.  The default is False.\n",
    "        method : str\n",
    "            Can either be 'nm' for Nelder-Mead or 'powell' for Powell.  The\n",
    "            optimization method that optimizes over nuisance parameters.\n",
    "            The default is 'nm'.\n",
    "        stochastic_exog : bool\n",
    "            When True, the exogenous variables are assumed to be stochastic.\n",
    "            When the regressors are nonstochastic, moment conditions are\n",
    "            placed on the exogenous variables.  Confidence intervals for\n",
    "            stochastic regressors are at least as large as non-stochastic\n",
    "            regressors. The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            The p-value and -2 times the log-likelihood ratio for the\n",
    "            hypothesized values.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        >>> import statsmodels.api as sm\n",
    "        >>> data = sm.datasets.stackloss.load()\n",
    "        >>> endog = data.endog\n",
    "        >>> exog = sm.add_constant(data.exog)\n",
    "        >>> model = sm.OLS(endog, exog)\n",
    "        >>> fitted = model.fit()\n",
    "        >>> fitted.params\n",
    "        >>> array([-39.91967442,   0.7156402 ,   1.29528612,  -0.15212252])\n",
    "        >>> fitted.rsquared\n",
    "        >>> 0.91357690446068196\n",
    "        >>> # Test that the slope on the first variable is 0\n",
    "        >>> fitted.el_test([0], [1])\n",
    "        >>> (27.248146353888796, 1.7894660442330235e-07)\n",
    "        \"\"\"\n",
    "        params = np.copy(self.params)\n",
    "        opt_fun_inst = _ELRegOpts()  # to store weights\n",
    "        if len(param_nums) == len(params):\n",
    "            llr = opt_fun_inst._opt_nuis_regress(\n",
    "                [],\n",
    "                param_nums=param_nums,\n",
    "                endog=self.model.endog,\n",
    "                exog=self.model.exog,\n",
    "                nobs=self.model.nobs,\n",
    "                nvar=self.model.exog.shape[1],\n",
    "                params=params,\n",
    "                b0_vals=b0_vals,\n",
    "                stochastic_exog=stochastic_exog)\n",
    "            pval = 1 - stats.chi2.cdf(llr, len(param_nums))\n",
    "            if return_weights:\n",
    "                return llr, pval, opt_fun_inst.new_weights\n",
    "            else:\n",
    "                return llr, pval\n",
    "        x0 = np.delete(params, param_nums)\n",
    "        args = (param_nums, self.model.endog, self.model.exog,\n",
    "                self.model.nobs, self.model.exog.shape[1], params,\n",
    "                b0_vals, stochastic_exog)\n",
    "        if method == 'nm':\n",
    "            llr = optimize.fmin(opt_fun_inst._opt_nuis_regress, x0,\n",
    "                                maxfun=10000, maxiter=10000, full_output=1,\n",
    "                                disp=0, args=args)[1]\n",
    "        if method == 'powell':\n",
    "            llr = optimize.fmin_powell(opt_fun_inst._opt_nuis_regress, x0,\n",
    "                                       full_output=1, disp=0,\n",
    "                                       args=args)[1]\n",
    "\n",
    "        pval = 1 - stats.chi2.cdf(llr, len(param_nums))\n",
    "        if ret_params:\n",
    "            return llr, pval, opt_fun_inst.new_weights, opt_fun_inst.new_params\n",
    "        elif return_weights:\n",
    "            return llr, pval, opt_fun_inst.new_weights\n",
    "        else:\n",
    "            return llr, pval\n",
    "\n",
    "    def conf_int_el(self, param_num, sig=.05, upper_bound=None,\n",
    "                    lower_bound=None, method='nm', stochastic_exog=True):\n",
    "        \"\"\"\n",
    "        Compute the confidence interval using Empirical Likelihood.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        param_num : float\n",
    "            The parameter for which the confidence interval is desired.\n",
    "        sig : float\n",
    "            The significance level.  Default is 0.05.\n",
    "        upper_bound : float\n",
    "            The maximum value the upper limit can be.  Default is the\n",
    "            99.9% confidence value under OLS assumptions.\n",
    "        lower_bound : float\n",
    "            The minimum value the lower limit can be.  Default is the 99.9%\n",
    "            confidence value under OLS assumptions.\n",
    "        method : str\n",
    "            Can either be 'nm' for Nelder-Mead or 'powell' for Powell.  The\n",
    "            optimization method that optimizes over nuisance parameters.\n",
    "            The default is 'nm'.\n",
    "        stochastic_exog : bool\n",
    "            When True, the exogenous variables are assumed to be stochastic.\n",
    "            When the regressors are nonstochastic, moment conditions are\n",
    "            placed on the exogenous variables.  Confidence intervals for\n",
    "            stochastic regressors are at least as large as non-stochastic\n",
    "            regressors.  The default is True.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        lowerl : float\n",
    "            The lower bound of the confidence interval.\n",
    "        upperl : float\n",
    "            The upper bound of the confidence interval.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        el_test : Test parameters using Empirical Likelihood.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        This function uses brentq to find the value of beta where\n",
    "        test_beta([beta], param_num)[1] is equal to the critical value.\n",
    "\n",
    "        The function returns the results of each iteration of brentq at each\n",
    "        value of beta.\n",
    "\n",
    "        The current function value of the last printed optimization should be\n",
    "        the critical value at the desired significance level. For alpha=.05,\n",
    "        the value is 3.841459.\n",
    "\n",
    "        To ensure optimization terminated successfully, it is suggested to do\n",
    "        el_test([lower_limit], [param_num]).\n",
    "\n",
    "        If the optimization does not terminate successfully, consider switching\n",
    "        optimization algorithms.\n",
    "\n",
    "        If optimization is still not successful, try changing the values of\n",
    "        start_int_params.  If the current function value repeatedly jumps\n",
    "        from a number between 0 and the critical value and a very large number\n",
    "        (>50), the starting parameters of the interior minimization need\n",
    "        to be changed.\n",
    "        \"\"\"\n",
    "        r0 = stats.chi2.ppf(1 - sig, 1)\n",
    "        if upper_bound is None:\n",
    "            upper_bound = self.conf_int(.01)[param_num][1]\n",
    "        if lower_bound is None:\n",
    "            lower_bound = self.conf_int(.01)[param_num][0]\n",
    "\n",
    "        def f(b0):\n",
    "            return self.el_test(np.array([b0]), np.array([param_num]),\n",
    "                                method=method,\n",
    "                                stochastic_exog=stochastic_exog)[0] - r0\n",
    "\n",
    "        lowerl = optimize.brenth(f, lower_bound,\n",
    "                                 self.params[param_num])\n",
    "        upperl = optimize.brenth(f, self.params[param_num],\n",
    "                                 upper_bound)\n",
    "        #  ^ Seems to be faster than brentq in most cases\n",
    "        return (lowerl, upperl)\n",
    "\n",
    "\n",
    "class RegressionResultsWrapper(wrap.ResultsWrapper):\n",
    "\n",
    "    _attrs = {\n",
    "        'chisq': 'columns',\n",
    "        'sresid': 'rows',\n",
    "        'weights': 'rows',\n",
    "        'wresid': 'rows',\n",
    "        'bcov_unscaled': 'cov',\n",
    "        'bcov_scaled': 'cov',\n",
    "        'HC0_se': 'columns',\n",
    "        'HC1_se': 'columns',\n",
    "        'HC2_se': 'columns',\n",
    "        'HC3_se': 'columns',\n",
    "        'norm_resid': 'rows',\n",
    "    }\n",
    "\n",
    "    _wrap_attrs = wrap.union_dicts(base.LikelihoodResultsWrapper._attrs,\n",
    "                                   _attrs)\n",
    "\n",
    "    _methods = {}\n",
    "\n",
    "    _wrap_methods = wrap.union_dicts(\n",
    "                        base.LikelihoodResultsWrapper._wrap_methods,\n",
    "                        _methods)\n",
    "\n",
    "\n",
    "wrap.populate_wrapper(RegressionResultsWrapper,\n",
    "                      RegressionResults)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
