{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'OLS' has no attribute 'summary'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msm\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(inspect\u001b[38;5;241m.\u001b[39mgetmodule(\u001b[43msm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOLS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()))\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'OLS' has no attribute 'summary'"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import statsmodels.api as sm\n",
    "\n",
    "print(inspect.getmodule(sm.OLS.summary()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whiten(x):\n",
    "    x = np.asarray(x)\n",
    "    return np.dot(x, self.cholsigmainv.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nobs(self):\n",
    "    \"\"\"Number of observations n.\"\"\"\n",
    "    return float(self.model.wexog.shape[0])\n",
    "\n",
    "def resid(self):\n",
    "    \"\"\"The residuals of the model.\"\"\"\n",
    "    return self.model.endog - self.model.predict(\n",
    "        self.params, self.model.exog)\n",
    "\n",
    "\n",
    "def scale(self):\n",
    "    \"\"\"\n",
    "    A scale factor for the covariance matrix.\n",
    "\n",
    "    The Default value is ssr/(n-p).  Note that the square root of `scale`\n",
    "    is often called the standard error of the regression.\n",
    "    \"\"\"\n",
    "    wresid = self.wresid\n",
    "    return np.dot(wresid, wresid) / self.df_resid\n",
    "\n",
    "def ssr(self):\n",
    "    \"\"\"Sum of squared (whitened) residuals.\"\"\"\n",
    "    wresid = self.wresid\n",
    "    return np.dot(wresid, wresid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionModel(base.LikelihoodModel):\n",
    "    \"\"\"\n",
    "    Base class for linear regression models. Should not be directly called.\n",
    "\n",
    "    Intended for subclassing.\n",
    "    \"\"\"\n",
    "    def __init__(self, endog, exog, **kwargs):\n",
    "        super().__init__(endog, exog, **kwargs)\n",
    "        self.pinv_wexog: Float64Array | None = None\n",
    "        self._data_attr.extend(['pinv_wexog', 'wendog', 'wexog', 'weights'])\n",
    "\n",
    "    def initialize(self):\n",
    "        \"\"\"Initialize model components.\"\"\"\n",
    "        self.wexog = self.whiten(self.exog)\n",
    "        self.wendog = self.whiten(self.endog)\n",
    "        # overwrite nobs from class Model:\n",
    "        self.nobs = float(self.wexog.shape[0])\n",
    "\n",
    "        self._df_model = None\n",
    "        self._df_resid = None\n",
    "        self.rank = None\n",
    "\n",
    "    @property\n",
    "    def df_model(self):\n",
    "        \"\"\"\n",
    "        The model degree of freedom.\n",
    "\n",
    "        The dof is defined as the rank of the regressor matrix minus 1 if a\n",
    "        constant is included.\n",
    "        \"\"\"\n",
    "        if self._df_model is None:\n",
    "            if self.rank is None:\n",
    "                self.rank = np.linalg.matrix_rank(self.exog)\n",
    "            self._df_model = float(self.rank - self.k_constant)\n",
    "        return self._df_model\n",
    "\n",
    "    @df_model.setter\n",
    "    def df_model(self, value):\n",
    "        self._df_model = value\n",
    "\n",
    "    @property\n",
    "    def df_resid(self):\n",
    "        \"\"\"\n",
    "        The residual degree of freedom.\n",
    "\n",
    "        The dof is defined as the number of observations minus the rank of\n",
    "        the regressor matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        if self._df_resid is None:\n",
    "            if self.rank is None:\n",
    "                self.rank = np.linalg.matrix_rank(self.exog)\n",
    "            self._df_resid = self.nobs - self.rank\n",
    "        return self._df_resid\n",
    "\n",
    "    @df_resid.setter\n",
    "    def df_resid(self, value):\n",
    "        self._df_resid = value\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        Whiten method that must be overwritten by individual models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement.\")\n",
    "\n",
    "    def fit(\n",
    "            self,\n",
    "            method: Literal[\"pinv\", \"qr\"] = \"pinv\",\n",
    "            cov_type: Literal[\n",
    "                \"nonrobust\",\n",
    "                \"fixed scale\",\n",
    "                \"HC0\",\n",
    "                \"HC1\",\n",
    "                \"HC2\",\n",
    "                \"HC3\",\n",
    "                \"HAC\",\n",
    "                \"hac-panel\",\n",
    "                \"hac-groupsum\",\n",
    "                \"cluster\",\n",
    "            ] = \"nonrobust\",\n",
    "            cov_kwds=None,\n",
    "            use_t: bool | None = None,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Full fit of the model.\n",
    "\n",
    "        The results include an estimate of covariance matrix, (whitened)\n",
    "        residuals and an estimate of scale.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        method : str, optional\n",
    "            Can be \"pinv\", \"qr\".  \"pinv\" uses the Moore-Penrose pseudoinverse\n",
    "            to solve the least squares problem. \"qr\" uses the QR\n",
    "            factorization.\n",
    "        cov_type : str, optional\n",
    "            See `regression.linear_model.RegressionResults` for a description\n",
    "            of the available covariance estimators.\n",
    "        cov_kwds : list or None, optional\n",
    "            See `linear_model.RegressionResults.get_robustcov_results` for a\n",
    "            description required keywords for alternative covariance\n",
    "            estimators.\n",
    "        use_t : bool, optional\n",
    "            Flag indicating to use the Student's t distribution when computing\n",
    "            p-values.  Default behavior depends on cov_type. See\n",
    "            `linear_model.RegressionResults.get_robustcov_results` for\n",
    "            implementation details.\n",
    "        **kwargs\n",
    "            Additional keyword arguments that contain information used when\n",
    "            constructing a model using the formula interface.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        RegressionResults\n",
    "            The model estimation results.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        RegressionResults\n",
    "            The results container.\n",
    "        RegressionResults.get_robustcov_results\n",
    "            A method to change the covariance estimator used when fitting the\n",
    "            model.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        The fit method uses the pseudoinverse of the design/exogenous variables\n",
    "        to solve the least squares minimization.\n",
    "        \"\"\"\n",
    "        if method == \"pinv\":\n",
    "            if not (hasattr(self, 'pinv_wexog') and\n",
    "                    hasattr(self, 'normalized_cov_params') and\n",
    "                    hasattr(self, 'rank')):\n",
    "\n",
    "                self.pinv_wexog, singular_values = pinv_extended(self.wexog)\n",
    "                self.normalized_cov_params = np.dot(\n",
    "                    self.pinv_wexog, np.transpose(self.pinv_wexog))\n",
    "\n",
    "                # Cache these singular values for use later.\n",
    "                self.wexog_singular_values = singular_values\n",
    "                self.rank = np.linalg.matrix_rank(np.diag(singular_values))\n",
    "\n",
    "            beta = np.dot(self.pinv_wexog, self.wendog)\n",
    "\n",
    "        elif method == \"qr\":\n",
    "            if not (hasattr(self, 'exog_Q') and\n",
    "                    hasattr(self, 'exog_R') and\n",
    "                    hasattr(self, 'normalized_cov_params') and\n",
    "                    hasattr(self, 'rank')):\n",
    "                Q, R = np.linalg.qr(self.wexog)\n",
    "                self.exog_Q, self.exog_R = Q, R\n",
    "                self.normalized_cov_params = np.linalg.inv(np.dot(R.T, R))\n",
    "\n",
    "                # Cache singular values from R.\n",
    "                self.wexog_singular_values = np.linalg.svd(R, 0, 0)\n",
    "                self.rank = np.linalg.matrix_rank(R)\n",
    "            else:\n",
    "                Q, R = self.exog_Q, self.exog_R\n",
    "            # Needed for some covariance estimators, see GH #8157\n",
    "            self.pinv_wexog = np.linalg.pinv(self.wexog)\n",
    "            # used in ANOVA\n",
    "            self.effects = effects = np.dot(Q.T, self.wendog)\n",
    "            beta = np.linalg.solve(R, effects)\n",
    "        else:\n",
    "            raise ValueError('method has to be \"pinv\" or \"qr\"')\n",
    "\n",
    "        if self._df_model is None:\n",
    "            self._df_model = float(self.rank - self.k_constant)\n",
    "        if self._df_resid is None:\n",
    "            self.df_resid = self.nobs - self.rank\n",
    "\n",
    "        if isinstance(self, OLS):\n",
    "            lfit = OLSResults(\n",
    "                self, beta,\n",
    "                normalized_cov_params=self.normalized_cov_params,\n",
    "                cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t)\n",
    "        else:\n",
    "            lfit = RegressionResults(\n",
    "                self, beta,\n",
    "                normalized_cov_params=self.normalized_cov_params,\n",
    "                cov_type=cov_type, cov_kwds=cov_kwds, use_t=use_t,\n",
    "                **kwargs)\n",
    "        return RegressionResultsWrapper(lfit)\n",
    "\n",
    "    def predict(self, params, exog=None):\n",
    "        \"\"\"\n",
    "        Return linear predicted values from a design matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            Parameters of a linear model.\n",
    "        exog : array_like, optional\n",
    "            Design / exogenous data. Model exog is used if None.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            An array of fitted values.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        If the model has not yet been fit, params is not optional.\n",
    "        \"\"\"\n",
    "        # JP: this does not look correct for GLMAR\n",
    "        # SS: it needs its own predict method\n",
    "\n",
    "        if exog is None:\n",
    "            exog = self.exog\n",
    "\n",
    "        return np.dot(exog, params)\n",
    "\n",
    "    def get_distribution(self, params, scale, exog=None, dist_class=None):\n",
    "        \"\"\"\n",
    "        Construct a random number generator for the predictive distribution.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The model parameters (regression coefficients).\n",
    "        scale : scalar\n",
    "            The variance parameter.\n",
    "        exog : array_like\n",
    "            The predictor variable matrix.\n",
    "        dist_class : class\n",
    "            A random number generator class.  Must take 'loc' and 'scale'\n",
    "            as arguments and return a random number generator implementing\n",
    "            an ``rvs`` method for simulating random values. Defaults to normal.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        gen\n",
    "            Frozen random number generator object with mean and variance\n",
    "            determined by the fitted linear model.  Use the ``rvs`` method\n",
    "            to generate random values.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Due to the behavior of ``scipy.stats.distributions objects``,\n",
    "        the returned random number generator must be called with\n",
    "        ``gen.rvs(n)`` where ``n`` is the number of observations in\n",
    "        the data set used to fit the model.  If any other value is\n",
    "        used for ``n``, misleading results will be produced.\n",
    "        \"\"\"\n",
    "        fit = self.predict(params, exog)\n",
    "        if dist_class is None:\n",
    "            from scipy.stats.distributions import norm\n",
    "            dist_class = norm\n",
    "        gen = dist_class(loc=fit, scale=np.sqrt(scale))\n",
    "        return gen\n",
    "\n",
    "class WLS(RegressionModel):\n",
    "    __doc__ = \"\"\"\n",
    "    Weighted Least Squares\n",
    "\n",
    "    The weights are presumed to be (proportional to) the inverse of\n",
    "    the variance of the observations.  That is, if the variables are\n",
    "    to be transformed by 1/sqrt(W) you must supply weights = 1/W.\n",
    "\n",
    "    {params}\n",
    "    weights : array_like, optional\n",
    "        A 1d array of weights.  If you supply 1/W then the variables are\n",
    "        pre- multiplied by 1/sqrt(W).  If no weights are supplied the\n",
    "        default value is 1 and WLS results are the same as OLS.\n",
    "    {extra_params}\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : ndarray\n",
    "        The stored weights supplied as an argument.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    GLS : Fit a linear model using Generalized Least Squares.\n",
    "    OLS : Fit a linear model using Ordinary Least Squares.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If the weights are a function of the data, then the post estimation\n",
    "    statistics such as fvalue and mse_model might not be correct, as the\n",
    "    package does not yet support no-constant regression.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import statsmodels.api as sm\n",
    "    >>> Y = [1,3,4,5,2,3,4]\n",
    "    >>> X = range(1,8)\n",
    "    >>> X = sm.add_constant(X)\n",
    "    >>> wls_model = sm.WLS(Y,X, weights=list(range(1,8)))\n",
    "    >>> results = wls_model.fit()\n",
    "    >>> results.params\n",
    "    array([ 2.91666667,  0.0952381 ])\n",
    "    >>> results.tvalues\n",
    "    array([ 2.0652652 ,  0.35684428])\n",
    "    >>> print(results.t_test([1, 0]))\n",
    "    <T test: effect=array([ 2.91666667]), sd=array([[ 1.41224801]]),\n",
    "     t=array([[ 2.0652652]]), p=array([[ 0.04690139]]), df_denom=5>\n",
    "    >>> print(results.f_test([0, 1]))\n",
    "    <F test: F=array([[ 0.12733784]]), p=[[ 0.73577409]], df_denom=5, df_num=1>\n",
    "    \"\"\".format(params=base._model_params_doc,\n",
    "           extra_params=base._missing_param_doc + base._extra_param_doc)\n",
    "\n",
    "    def __init__(self, endog, exog, weights=1., missing='none', hasconst=None,\n",
    "                 **kwargs):\n",
    "        if type(self) is WLS:\n",
    "            self._check_kwargs(kwargs)\n",
    "        weights = np.array(weights)\n",
    "        if weights.shape == ():\n",
    "            if (missing == 'drop' and 'missing_idx' in kwargs and\n",
    "                    kwargs['missing_idx'] is not None):\n",
    "                # patsy may have truncated endog\n",
    "                weights = np.repeat(weights, len(kwargs['missing_idx']))\n",
    "            else:\n",
    "                weights = np.repeat(weights, len(endog))\n",
    "        # handle case that endog might be of len == 1\n",
    "        if len(weights) == 1:\n",
    "            weights = np.array([weights.squeeze()])\n",
    "        else:\n",
    "            weights = weights.squeeze()\n",
    "        super().__init__(endog, exog, missing=missing,\n",
    "                                  weights=weights, hasconst=hasconst, **kwargs)\n",
    "        nobs = self.exog.shape[0]\n",
    "        weights = self.weights\n",
    "        if weights.size != nobs and weights.shape[0] != nobs:\n",
    "            raise ValueError('Weights must be scalar or same length as design')\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        Whitener for WLS model, multiplies each column by sqrt(self.weights).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The whitened values sqrt(weights)*X.\n",
    "        \"\"\"\n",
    "\n",
    "        x = np.asarray(x)\n",
    "        if x.ndim == 1:\n",
    "            return x * np.sqrt(self.weights)\n",
    "        elif x.ndim == 2:\n",
    "            return np.sqrt(self.weights)[:, None] * x\n",
    "\n",
    "    def loglike(self, params):\n",
    "        r\"\"\"\n",
    "        Compute the value of the gaussian log-likelihood function at params.\n",
    "\n",
    "        Given the whitened design matrix, the log-likelihood is evaluated\n",
    "        at the parameter vector `params` for the dependent variable `Y`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter estimates.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The value of the log-likelihood function for a WLS Model.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        .. math:: -\\frac{n}{2}\\log SSR\n",
    "                  -\\frac{n}{2}\\left(1+\\log\\left(\\frac{2\\pi}{n}\\right)\\right)\n",
    "                  +\\frac{1}{2}\\log\\left(\\left|W\\right|\\right)\n",
    "\n",
    "        where :math:`W` is a diagonal weight matrix,\n",
    "        :math:`\\left|W\\right|` is its determinant, and\n",
    "        :math:`SSR=\\left(Y-\\hat{Y}\\right)^\\prime W \\left(Y-\\hat{Y}\\right)` is\n",
    "        the sum of the squared weighted residuals.\n",
    "        \"\"\"\n",
    "        nobs2 = self.nobs / 2.0\n",
    "        SSR = np.sum((self.wendog - np.dot(self.wexog, params))**2, axis=0)\n",
    "        llf = -np.log(SSR) * nobs2      # concentrated likelihood\n",
    "        llf -= (1+np.log(np.pi/nobs2))*nobs2  # with constant\n",
    "        llf += 0.5 * np.sum(np.log(self.weights))\n",
    "        return llf\n",
    "\n",
    "    def hessian_factor(self, params, scale=None, observed=True):\n",
    "        \"\"\"\n",
    "        Compute the weights for calculating the Hessian.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            The parameter at which Hessian is evaluated.\n",
    "        scale : None or float\n",
    "            If scale is None, then the default scale will be calculated.\n",
    "            Default scale is defined by `self.scaletype` and set in fit.\n",
    "            If scale is not None, then it is used as a fixed scale.\n",
    "        observed : bool\n",
    "            If True, then the observed Hessian is returned. If false then the\n",
    "            expected information matrix is returned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            A 1d weight vector used in the calculation of the Hessian.\n",
    "            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.weights\n",
    "\n",
    "    @Appender(_fit_regularized_doc)\n",
    "    def fit_regularized(self, method=\"elastic_net\", alpha=0.,\n",
    "                        L1_wt=1., start_params=None, profile_scale=False,\n",
    "                        refit=False, **kwargs):\n",
    "        # Docstring attached below\n",
    "        if not np.isscalar(alpha):\n",
    "            alpha = np.asarray(alpha)\n",
    "        # Need to adjust since RSS/n in elastic net uses nominal n in\n",
    "        # denominator\n",
    "        alpha = alpha * np.sum(self.weights) / len(self.weights)\n",
    "\n",
    "        rslt = OLS(self.wendog, self.wexog).fit_regularized(\n",
    "            method=method, alpha=alpha,\n",
    "            L1_wt=L1_wt,\n",
    "            start_params=start_params,\n",
    "            profile_scale=profile_scale,\n",
    "            refit=refit, **kwargs)\n",
    "\n",
    "        from statsmodels.base.elastic_net import (\n",
    "            RegularizedResults,\n",
    "            RegularizedResultsWrapper,\n",
    "        )\n",
    "        rrslt = RegularizedResults(self, rslt.params)\n",
    "        return RegularizedResultsWrapper(rrslt)\n",
    "\n",
    "\n",
    "class OLS(WLS):\n",
    "    __doc__ = \"\"\"\n",
    "    Ordinary Least Squares\n",
    "\n",
    "    {params}\n",
    "    {extra_params}\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    weights : scalar\n",
    "        Has an attribute weights = array(1.0) due to inheritance from WLS.\n",
    "\n",
    "    See Also\n",
    "    --------\n",
    "    WLS : Fit a linear model using Weighted Least Squares.\n",
    "    GLS : Fit a linear model using Generalized Least Squares.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    No constant is added by the model unless you are using formulas.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import statsmodels.api as sm\n",
    "    >>> import numpy as np\n",
    "    >>> duncan_prestige = sm.datasets.get_rdataset(\"Duncan\", \"carData\")\n",
    "    >>> Y = duncan_prestige.data['income']\n",
    "    >>> X = duncan_prestige.data['education']\n",
    "    >>> X = sm.add_constant(X)\n",
    "    >>> model = sm.OLS(Y,X)\n",
    "    >>> results = model.fit()\n",
    "    >>> results.params\n",
    "    const        10.603498\n",
    "    education     0.594859\n",
    "    dtype: float64\n",
    "\n",
    "    >>> results.tvalues\n",
    "    const        2.039813\n",
    "    education    6.892802\n",
    "    dtype: float64\n",
    "\n",
    "    >>> print(results.t_test([1, 0]))\n",
    "                                 Test for Constraints\n",
    "    ==============================================================================\n",
    "                     coef    std err          t      P>|t|      [0.025      0.975]\n",
    "    ------------------------------------------------------------------------------\n",
    "    c0            10.6035      5.198      2.040      0.048       0.120      21.087\n",
    "    ==============================================================================\n",
    "\n",
    "    >>> print(results.f_test(np.identity(2)))\n",
    "    <F test: F=array([[159.63031026]]), p=1.2607168903696672e-20,\n",
    "     df_denom=43, df_num=2>\n",
    "    \"\"\".format(params=base._model_params_doc,\n",
    "           extra_params=base._missing_param_doc + base._extra_param_doc)\n",
    "\n",
    "    def __init__(self, endog, exog=None, missing='none', hasconst=None,\n",
    "                 **kwargs):\n",
    "        if \"weights\" in kwargs:\n",
    "            msg = (\"Weights are not supported in OLS and will be ignored\"\n",
    "                   \"An exception will be raised in the next version.\")\n",
    "            warnings.warn(msg, ValueWarning)\n",
    "        super().__init__(endog, exog, missing=missing,\n",
    "                                  hasconst=hasconst, **kwargs)\n",
    "        if \"weights\" in self._init_keys:\n",
    "            self._init_keys.remove(\"weights\")\n",
    "\n",
    "        if type(self) is OLS:\n",
    "            self._check_kwargs(kwargs, [\"offset\"])\n",
    "\n",
    "    def loglike(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        The likelihood function for the OLS model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The coefficients with which to estimate the log-likelihood.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float\n",
    "            The likelihood function evaluated at params.\n",
    "        \"\"\"\n",
    "        nobs2 = self.nobs / 2.0\n",
    "        nobs = float(self.nobs)\n",
    "        resid = self.endog - np.dot(self.exog, params)\n",
    "        if hasattr(self, 'offset'):\n",
    "            resid -= self.offset\n",
    "        ssr = np.sum(resid**2)\n",
    "        if scale is None:\n",
    "            # profile log likelihood\n",
    "            llf = -nobs2*np.log(2*np.pi) - nobs2*np.log(ssr / nobs) - nobs2\n",
    "        else:\n",
    "            # log-likelihood\n",
    "            llf = -nobs2 * np.log(2 * np.pi * scale) - ssr / (2*scale)\n",
    "        return llf\n",
    "\n",
    "    def whiten(self, x):\n",
    "        \"\"\"\n",
    "        OLS model whitener does nothing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : array_like\n",
    "            Data to be whitened.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        array_like\n",
    "            The input array unmodified.\n",
    "\n",
    "        See Also\n",
    "        --------\n",
    "        OLS : Fit a linear model using Ordinary Least Squares.\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "    def score(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        Evaluate the score function at a given point.\n",
    "\n",
    "        The score corresponds to the profile (concentrated)\n",
    "        log-likelihood in which the scale parameter has been profiled\n",
    "        out.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter vector at which the score function is\n",
    "            computed.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The score vector.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_wexog_xprod\"):\n",
    "            self._setup_score_hess()\n",
    "\n",
    "        xtxb = np.dot(self._wexog_xprod, params)\n",
    "        sdr = -self._wexog_x_wendog + xtxb\n",
    "\n",
    "        if scale is None:\n",
    "            ssr = self._wendog_xprod - 2 * np.dot(self._wexog_x_wendog.T,\n",
    "                                                  params)\n",
    "            ssr += np.dot(params, xtxb)\n",
    "            return -self.nobs * sdr / ssr\n",
    "        else:\n",
    "            return -sdr / scale\n",
    "\n",
    "    def _setup_score_hess(self):\n",
    "        y = self.wendog\n",
    "        if hasattr(self, 'offset'):\n",
    "            y = y - self.offset\n",
    "        self._wendog_xprod = np.sum(y * y)\n",
    "        self._wexog_xprod = np.dot(self.wexog.T, self.wexog)\n",
    "        self._wexog_x_wendog = np.dot(self.wexog.T, y)\n",
    "\n",
    "    def hessian(self, params, scale=None):\n",
    "        \"\"\"\n",
    "        Evaluate the Hessian function at a given point.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : array_like\n",
    "            The parameter vector at which the Hessian is computed.\n",
    "        scale : float or None\n",
    "            If None, return the profile (concentrated) log likelihood\n",
    "            (profiled over the scale parameter), else return the\n",
    "            log-likelihood using the given scale value.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            The Hessian matrix.\n",
    "        \"\"\"\n",
    "\n",
    "        if not hasattr(self, \"_wexog_xprod\"):\n",
    "            self._setup_score_hess()\n",
    "\n",
    "        xtxb = np.dot(self._wexog_xprod, params)\n",
    "\n",
    "        if scale is None:\n",
    "            ssr = self._wendog_xprod - 2 * np.dot(self._wexog_x_wendog.T,\n",
    "                                                  params)\n",
    "            ssr += np.dot(params, xtxb)\n",
    "            ssrp = -2*self._wexog_x_wendog + 2*xtxb\n",
    "            hm = self._wexog_xprod / ssr - np.outer(ssrp, ssrp) / ssr**2\n",
    "            return -self.nobs * hm / 2\n",
    "        else:\n",
    "            return -self._wexog_xprod / scale\n",
    "\n",
    "    def hessian_factor(self, params, scale=None, observed=True):\n",
    "        \"\"\"\n",
    "        Calculate the weights for the Hessian.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : ndarray\n",
    "            The parameter at which Hessian is evaluated.\n",
    "        scale : None or float\n",
    "            If scale is None, then the default scale will be calculated.\n",
    "            Default scale is defined by `self.scaletype` and set in fit.\n",
    "            If scale is not None, then it is used as a fixed scale.\n",
    "        observed : bool\n",
    "            If True, then the observed Hessian is returned. If false then the\n",
    "            expected information matrix is returned.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        ndarray\n",
    "            A 1d weight vector used in the calculation of the Hessian.\n",
    "            The hessian is obtained by `(exog.T * hessian_factor).dot(exog)`.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.ones(self.exog.shape[0])\n",
    "\n",
    "    @Appender(_fit_regularized_doc)\n",
    "    def fit_regularized(self, method=\"elastic_net\", alpha=0.,\n",
    "                        L1_wt=1., start_params=None, profile_scale=False,\n",
    "                        refit=False, **kwargs):\n",
    "\n",
    "        # In the future we could add support for other penalties, e.g. SCAD.\n",
    "        if method not in (\"elastic_net\", \"sqrt_lasso\"):\n",
    "            msg = \"Unknown method '%s' for fit_regularized\" % method\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        # Set default parameters.\n",
    "        defaults = {\"maxiter\":  50, \"cnvrg_tol\": 1e-10,\n",
    "                    \"zero_tol\": 1e-8}\n",
    "        defaults.update(kwargs)\n",
    "\n",
    "        if method == \"sqrt_lasso\":\n",
    "            from statsmodels.base.elastic_net import (\n",
    "                RegularizedResults,\n",
    "                RegularizedResultsWrapper,\n",
    "            )\n",
    "            params = self._sqrt_lasso(alpha, refit, defaults[\"zero_tol\"])\n",
    "            results = RegularizedResults(self, params)\n",
    "            return RegularizedResultsWrapper(results)\n",
    "\n",
    "        from statsmodels.base.elastic_net import fit_elasticnet\n",
    "\n",
    "        if L1_wt == 0:\n",
    "            return self._fit_ridge(alpha)\n",
    "\n",
    "        # If a scale parameter is passed in, the non-profile\n",
    "        # likelihood (residual sum of squares divided by -2) is used,\n",
    "        # otherwise the profile likelihood is used.\n",
    "        if profile_scale:\n",
    "            loglike_kwds = {}\n",
    "            score_kwds = {}\n",
    "            hess_kwds = {}\n",
    "        else:\n",
    "            loglike_kwds = {\"scale\": 1}\n",
    "            score_kwds = {\"scale\": 1}\n",
    "            hess_kwds = {\"scale\": 1}\n",
    "\n",
    "        return fit_elasticnet(self, method=method,\n",
    "                              alpha=alpha,\n",
    "                              L1_wt=L1_wt,\n",
    "                              start_params=start_params,\n",
    "                              loglike_kwds=loglike_kwds,\n",
    "                              score_kwds=score_kwds,\n",
    "                              hess_kwds=hess_kwds,\n",
    "                              refit=refit,\n",
    "                              check_step=False,\n",
    "                              **defaults)\n",
    "\n",
    "    def _sqrt_lasso(self, alpha, refit, zero_tol):\n",
    "\n",
    "        try:\n",
    "            import cvxopt\n",
    "        except ImportError:\n",
    "            msg = 'sqrt_lasso fitting requires the cvxopt module'\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        n = len(self.endog)\n",
    "        p = self.exog.shape[1]\n",
    "\n",
    "        h0 = cvxopt.matrix(0., (2*p+1, 1))\n",
    "        h1 = cvxopt.matrix(0., (n+1, 1))\n",
    "        h1[1:, 0] = cvxopt.matrix(self.endog, (n, 1))\n",
    "\n",
    "        G0 = cvxopt.spmatrix([], [], [], (2*p+1, 2*p+1))\n",
    "        for i in range(1, 2*p+1):\n",
    "            G0[i, i] = -1\n",
    "        G1 = cvxopt.matrix(0., (n+1, 2*p+1))\n",
    "        G1[0, 0] = -1\n",
    "        G1[1:, 1:p+1] = self.exog\n",
    "        G1[1:, p+1:] = -self.exog\n",
    "\n",
    "        c = cvxopt.matrix(alpha / n, (2*p + 1, 1))\n",
    "        c[0] = 1 / np.sqrt(n)\n",
    "\n",
    "        from cvxopt import solvers\n",
    "        solvers.options[\"show_progress\"] = False\n",
    "\n",
    "        rslt = solvers.socp(c, Gl=G0, hl=h0, Gq=[G1], hq=[h1])\n",
    "        x = np.asarray(rslt['x']).flat\n",
    "        bp = x[1:p+1]\n",
    "        bn = x[p+1:]\n",
    "        params = bp - bn\n",
    "\n",
    "        if not refit:\n",
    "            return params\n",
    "\n",
    "        ii = np.flatnonzero(np.abs(params) > zero_tol)\n",
    "        rfr = OLS(self.endog, self.exog[:, ii]).fit()\n",
    "        params *= 0\n",
    "        params[ii] = rfr.params\n",
    "\n",
    "        return params\n",
    "\n",
    "    def _fit_ridge(self, alpha):\n",
    "        \"\"\"\n",
    "        Fit a linear model using ridge regression.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : scalar or array_like\n",
    "            The penalty weight.  If a scalar, the same penalty weight\n",
    "            applies to all variables in the model.  If a vector, it\n",
    "            must have the same length as `params`, and contains a\n",
    "            penalty weight for each coefficient.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        Equivalent to fit_regularized with L1_wt = 0 (but implemented\n",
    "        more efficiently).\n",
    "        \"\"\"\n",
    "\n",
    "        u, s, vt = np.linalg.svd(self.exog, 0)\n",
    "        v = vt.T\n",
    "        q = np.dot(u.T, self.endog) * s\n",
    "        s2 = s * s\n",
    "        if np.isscalar(alpha):\n",
    "            sd = s2 + alpha * self.nobs\n",
    "            params = q / sd\n",
    "            params = np.dot(v, params)\n",
    "        else:\n",
    "            alpha = np.asarray(alpha)\n",
    "            vtav = self.nobs * np.dot(vt, alpha[:, None] * v)\n",
    "            d = np.diag(vtav) + s2\n",
    "            np.fill_diagonal(vtav, d)\n",
    "            r = np.linalg.solve(vtav, q)\n",
    "            params = np.dot(v, r)\n",
    "\n",
    "        from statsmodels.base.elastic_net import RegularizedResults\n",
    "        return RegularizedResults(self, params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
